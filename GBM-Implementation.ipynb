{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d6a011-f0d8-4ef5-a70a-f65d4e58eef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install ucimlrepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f15a39-b328-4d4a-ac90-73ab7fb25095",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import queue\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e46540-7d0f-4a33-aaa9-e89ab723297e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to import dataset to python -- pasted from UCI ML repo \n",
    "\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "   \n",
    "default_of_credit_card_clients = fetch_ucirepo(id=350) \n",
    "\n",
    "default_cc = default_of_credit_card_clients.data.original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7631c22f-13b5-4b60-bdda-ef07d4652e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: One Hot Encoding qualitative features to be used for Regression Tree implementation\n",
    "def ohe_qual_feats(data):\n",
    "    qual_feat = ['X2', 'X3', 'X4']\n",
    "    for ql in qual_feat:\n",
    "        oh_test = data[[ql]]\n",
    "        ohe = OneHotEncoder(handle_unknown = 'ignore', sparse_output = False)\n",
    "        one_hot_encoded = ohe.fit_transform(oh_test)\n",
    "        one_hot_df = pd.DataFrame(one_hot_encoded, columns=ohe.get_feature_names_out([ql]))\n",
    "        data = pd.concat([data, one_hot_df], axis = 1)\n",
    "    data = data.drop(columns = ['X2', 'X3', 'X4'])\n",
    "    return data\n",
    "\n",
    "default_cc = ohe_qual_feats(default_cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4065766-48d2-44f7-94e3-8aee9b2b01e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating X and y tables from original table that is then split into a training and testing set\n",
    "\n",
    "X = default_cc.loc[:, default_cc.columns != 'Y'] \n",
    "y = default_cc.loc[:, 'Y'].to_frame(name = 'target')\n",
    "   \n",
    "default_of_credit_card_clients.data.original\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd0083d-c1e8-433e-b0d9-9609fc115702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating test_df which is the training data combined (so both the features and the target variable in one table)\n",
    "# This is to have the DTs to be created based on the true values (target) \n",
    "\n",
    "test_df = X_train\n",
    "test_df['target'] = y_train\n",
    "test_df = test_df.drop('ID', axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bae034-22df-4c6e-8d0b-65e2ca4467e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for different points in a continuous feature, I wanted to find the boundary that minimizes the residuals\n",
    "# this method gathers 9 points (percentiles from the data)\n",
    "# I chose to do this over gradient descent simply because it is lest costly and would bring minimal change to the optimal boundary \n",
    "\n",
    "def minimize_stats(data, feat, target):\n",
    "    quant_df = data[[feat]] \n",
    "    qs = np.array(quant_df.quantile([0.1, 0.25, 0.3, 0.4, 0.5, 0.6, 0.75, 0.8, 0.9])[feat])\n",
    "    return qs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19ce314-3ad4-40ad-bec4-6f960c4ce657",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# need this particular formula to convert leaf outputs to predicted probabilities\n",
    "\n",
    "def lodds_to_prob(l_odds):\n",
    "    pred_prob =  np.exp(l_odds) / (1 + np.exp(l_odds))\n",
    "    return pred_prob\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0509d7d3-2247-4cd0-adf3-36cd3b834e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets the residual reduction from a particular split of a feature\n",
    "# weights False Negatives to be 4 times \"more\" than False Positives\n",
    "# encourages residual trees to predict true positives correctly \n",
    "\n",
    "def get_rss(data, feat, target, bounds):\n",
    "    rss_arr = np.empty(len(bounds))\n",
    "    grad_arr = np.empty(len(bounds))\n",
    "    \n",
    "    for i, b in enumerate(bounds):\n",
    "        dat_l = data[data[feat] > b]\n",
    "        dat_u = data[data[feat] <= b]\n",
    "\n",
    "        mean_l = np.mean(dat_l[target])\n",
    "        mean_u = np.mean(dat_u[target])\n",
    "\n",
    "        rss_l = 0\n",
    "        rss_u = 0\n",
    "        \n",
    "        true_val_u = dat_u['target'].to_numpy()\n",
    "        true_val_l = dat_l['target'].to_numpy()\n",
    "\n",
    "        target_u = dat_u[target].to_numpy()\n",
    "        target_l = dat_l[target].to_numpy()\n",
    "        \n",
    "        for j, u in enumerate(true_val_u):\n",
    "            if u == 1:\n",
    "                rss_u += 4*((target_u[j] - mean_u)**2)\n",
    "            else:    \n",
    "                rss_u += ((target_u[j] - mean_u)**2)\n",
    "                \n",
    "        for k, l in enumerate(true_val_l):\n",
    "            if l == 1:\n",
    "                rss_l += 4*((target_l[k] - mean_l)**2)\n",
    "            else: \n",
    "                rss_l += ((target_l[k] - mean_l)**2)\n",
    "        \n",
    "        \n",
    "        \n",
    "        grad_l  = np.sum(-2*(dat_l[target]  - mean_l)) \n",
    "        grad_u = np.sum(-2*(dat_u[target] - mean_u))\n",
    "        \n",
    "        rss_arr[i] = rss_l + rss_u\n",
    "        grad_arr[i] = grad_l + grad_u\n",
    "\n",
    "    rss_min = np.min(rss_arr)\n",
    "    min_ind = np.argmin(rss_arr)\n",
    "    \n",
    "    # returns minimum rss, the optimal split, gradient, the feature name, and the orginal dataframe\n",
    "    return rss_min, bounds[min_ind], grad_arr[min_ind], feat, data\n",
    "\n",
    "get_rss(test_df, 'X6', 'target', [0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8a50f0-c436-4d07-87ab-15c7cdcdbfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each point/node in tree, find the feature that minimizes the residual sum of squares\n",
    "# this will select the feature/split for each DT node\n",
    "# inputs are the data at that particular node, and target represents the feature that the DT is trying to predict\n",
    "\n",
    "def minimal_rss_quant(data, target):\n",
    "    feats = ['X1', 'X5', 'X6', 'X7', 'X8', 'X9', 'X10', 'X11', 'X12', 'X13', 'X14',\n",
    "       'X15', 'X16', 'X17', 'X18', 'X19', 'X20', 'X21', 'X22', 'X23', 'X2_1',\n",
    "       'X2_2', 'X3_0', 'X3_1', 'X3_2', 'X3_3', 'X3_4', 'X3_5', 'X3_6', 'X4_0',\n",
    "       'X4_1', 'X4_2', 'X4_3', target]\n",
    "    dater = []\n",
    "    min_rss = np.inf\n",
    "    min_stats = None\n",
    "    \n",
    "    for f in feats:\n",
    "        if f != target:\n",
    "            bounds = minimize_stats(data, f, target)\n",
    "            stats = get_rss(data, f, target, bounds)\n",
    "            rss = stats[0]\n",
    "\n",
    "            if rss < min_rss:\n",
    "                min_rss = rss\n",
    "                min_stats = stats\n",
    "    return min_stats\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4903c6c-8294-4f02-90f4-9db9a800b8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created DTNode to represent each node of the DTs created, \n",
    "# DecisionTreeRegressor class helps traverse the tree, create the tree, and \n",
    "# identify which nodes are considered leaves (will be useful later) \n",
    "\n",
    "class DTNode: \n",
    "\n",
    "    def __init__(self):\n",
    "        self.value = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "\n",
    "\n",
    "class DecisionTreeRegressor:\n",
    "\n",
    "    def __init__(self, data, depth, target):\n",
    "        self.depth = depth  \n",
    "        self.data = data\n",
    "        self.target = target\n",
    "\n",
    "    def create_tree(self):\n",
    "        self.dtree = build_tree(self.data, self.depth, self.target)\n",
    "        return self.dtree\n",
    "\n",
    "    def isLeaf(dtnode):\n",
    "        return ((dtnode.right is None) and (dtnode.left is None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efdd9d8-14bf-46d3-acb1-89007cafc119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build_tree takes in the dataframe at this point (whatever split, the depth of the tree - as indicated when the DTRegressor class \n",
    "# is instantiated; uses recursion to continually split the new data split the most optimally, and continues until the indicated depth is reached\n",
    "\n",
    "def build_tree(data, depth, target):\n",
    "    if data.shape[0] > 0:\n",
    "        root = DTNode()\n",
    "        root.value = minimal_rss_quant(data, target)\n",
    "    else: \n",
    "        return 0 \n",
    " \n",
    "    if depth == 0:\n",
    "        return root  \n",
    "    else:\n",
    "        children_data = create_children(data, root)\n",
    "        \n",
    "        root.left = DTNode()\n",
    "        root.right = DTNode() \n",
    "\n",
    "        depth = depth - 1\n",
    "        root.left = build_tree(children_data[0], depth, target)\n",
    "        root.right = build_tree(children_data[1], depth, target)\n",
    "    return root\n",
    "\n",
    "# create_children is a helper function for build_tree in order to create new data frames upon splitting the data\n",
    "# e.g. if the feature to split on is X6, and the value is 1 then create_children takes the data and splits it into the following:\n",
    "# left node/dataframe : clients that have X6 <= 1 , right node/dataframe : clients that have X6 > 1\n",
    "\n",
    "def create_children(data, node):\n",
    "    feat_name = node.value[3]\n",
    "    pivot_val = node.value[1]\n",
    "    \n",
    "    left_data = data[data[feat_name] <= pivot_val]\n",
    "    right_data = data[data[feat_name] > pivot_val]\n",
    "\n",
    "    return left_data, right_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd3a1e6-aba1-4713-a2c6-708180bbd11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_new_val creates the new predictions for each leaf given the previous\n",
    "\n",
    "def get_new_val(data, resid, guess_col):\n",
    "    probs = lodds_to_prob(data[guess_col])\n",
    "    new_pred = np.sum(data[resid]) / np.sum(probs * (1 - probs))\n",
    "    return new_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcfe721-4995-44e5-8f2f-2bef2237b05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# leaf_arr[1]\n",
    "\n",
    "\n",
    "\n",
    "# for each decision tree,\n",
    "# find what value the output will predict\n",
    "# and then sum up those values F0 + v*sum blah blah (35:06)\n",
    "\n",
    "# Testing...\n",
    "\n",
    "# for each tree, store every node's split (less than + greater than/equal)\n",
    "# Run test data through the tree to get leaves, (in testing... store leaf values to be able to assign them to test)\n",
    "# Then repeat this with the next tree, with the predicted residuals/log(odds) from ^^^\n",
    "# have 2nd tree data stored, and assign leaves and repeat until dones with all trees\n",
    "# each observation shoudl now have some probability that it is 1 or 0, find probability threshold,, maybe use ROC or AUC to determine best threshold??\n",
    "# then assign final predicted values 0, 1\n",
    "# do model evaluations (confusion matrix, precision, recall, accuracy, F1 score, ROC curve\n",
    "\n",
    "#get_F0(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9072033-e59d-4f32-bc05-d0e8b6c17678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get initial prediction \n",
    "def get_F0(data): \n",
    "    p_1 = data[data['target'] == 1].shape[0] / len(data['target'])\n",
    "    odds = p_1 / (1 - p_1)\n",
    "    log_odds = np.log(odds)\n",
    "    return log_odds\n",
    "\n",
    "def lodds_to_prob(l_odds):\n",
    "    pred_prob =  np.exp(l_odds) / (1 + np.exp(l_odds))\n",
    "    return pred_prob\n",
    "\n",
    "class GBMInitializer:\n",
    "    def __init__(self, data, depth):\n",
    "        self.data = data\n",
    "        self.depth = depth\n",
    "        \n",
    "    def initialize_data(self):\n",
    "        lodds = get_F0(self.data)\n",
    "        prob_0 = lodds_to_prob(lodds)\n",
    "        self.data['F0'] = lodds\n",
    "        self.data['pred. prob 0'] = prob_0\n",
    "        self.data['residual 0'] = self.data['target'] - prob_0\n",
    "        return 0\n",
    "\n",
    "    def create_regression_tree(self, target):\n",
    "        fake_tree = DecisionTreeRegressor(self.data, self.depth, target)\n",
    "        doo = fake_tree.create_tree()\n",
    "        return doo\n",
    "\n",
    "    def create_model():\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779a8f0b-b1b1-4577-89c8-08535c938d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isLeaf(dtnode):\n",
    "    return ((dtnode.right is None) and (dtnode.left is None))\n",
    "    \n",
    "def returnLeaves(root):\n",
    "    leaf_arr = [] \n",
    "    def getLeaves(root_node): \n",
    "        if isLeaf(root_node):\n",
    "            leaf_arr.append([root_node.value[4]])\n",
    "        else:\n",
    "            getLeaves(root_node.left)\n",
    "            getLeaves(root_node.right)\n",
    "    getLeaves(root)\n",
    "    return leaf_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4c3d09-d77c-4fa8-9079-c331a557d126",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb28e337-ec05-4bef-8c0a-a3813667c7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_val(data, resid, probs):\n",
    "    new_pred = np.sum(data[resid]) / np.sum(data[probs] * (1 - data[probs]))\n",
    "    return new_pred\n",
    "\n",
    "\n",
    "class GradientBoostingMachine:\n",
    "    def __init__(self, data, num_learners, depth, dt_arr, learning_rate):\n",
    "        self.data = data\n",
    "        self.num_learners = num_learners\n",
    "        self.depth = depth\n",
    "        self.dt_arr = dt_arr\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def train(self): \n",
    "        gbm = GBMInitializer(self.data, self.depth)\n",
    "        gbm.initialize_data()\n",
    "        dt_tree = gbm.create_regression_tree('residual 0')\n",
    "        leaves = returnLeaves(dt_tree)\n",
    "\n",
    "        self.dt_arr.append(dt_tree)\n",
    "        pq_arr = []\n",
    "        for i in range(1, self.num_learners + 1):\n",
    "            new_leaves =[]\n",
    "            pq = queue.Queue()\n",
    "            for l in leaves:\n",
    "                l = l.copy()\n",
    "                new_pred = get_new_val(l[0], f'residual {i - 1}', f'pred. prob {i - 1}')\n",
    "                #l.append(new_pred) # new line.. see if this works\n",
    "                l[0].loc[:,f'F{i}'] = new_pred\n",
    "                new_leaves.append(l[0])\n",
    "                pq.put(new_pred)\n",
    "            \n",
    "            new_df = pd.concat(new_leaves, axis = 0)\n",
    "            odds = new_df[f'F{i - 1}'] + self.learning_rate*new_df[f'F{i}']\n",
    "            new_df[f'pred. prob {i}'] = lodds_to_prob(odds)\n",
    "            new_df[f'residual {i}'] = new_df['target'] - new_df[f'pred. prob {i}']\n",
    "\n",
    "            pq_arr.append(pq)\n",
    "            ore = GBMInitializer(new_df, 2)\n",
    "            dt_2 = ore.create_regression_tree(f'residual {i}')\n",
    "            self.dt_arr.append(dt_2)\n",
    "            leaves = returnLeaves(dt_2)\n",
    "        return self.dt_arr, new_df, pq_arr\n",
    "\n",
    "    \n",
    "    def train_next_batches(self, resid_num):\n",
    "        gbm = GBMInitializer(self.data, self.depth)\n",
    "        dt_tree2 = gbm.create_regression_tree(f'residual {resid_num}')\n",
    "        leaves = returnLeaves(dt_tree2)\n",
    "        \n",
    "        self.dt_arr.append(dt_tree2)\n",
    "        pq_arr2 = []\n",
    "        for i in range(resid_num + 1, resid_num + self.num_learners + 1):\n",
    "            new_leaves =[]\n",
    "            pq = queue.Queue()\n",
    "            for l in leaves:\n",
    "                l = l.copy()\n",
    "                new_pred = get_new_val(l[0], f'residual {i - 1}', f'pred. prob {i - 1}')\n",
    "                #l.append(new_pred) # new line.. see if this works\n",
    "                l[0].loc[:,f'F{i}'] = new_pred\n",
    "                new_leaves.append(l[0])\n",
    "                pq.put(new_pred)\n",
    "            \n",
    "            new_df = pd.concat(new_leaves, axis = 0)\n",
    "            odds = new_df[f'F{i - 1}'] + self.learning_rate*new_df[f'F{i}']\n",
    "            new_df[f'pred. prob {i}'] = lodds_to_prob(odds)\n",
    "            new_df[f'residual {i}'] = new_df['target'] - new_df[f'pred. prob {i}']\n",
    "\n",
    "            pq_arr2.append(pq)\n",
    "            ore = GBMInitializer(new_df, 2)\n",
    "            dt_2 = ore.create_regression_tree(f'residual {i}')\n",
    "            self.dt_arr.append(dt_2)\n",
    "            leaves = returnLeaves(dt_2)\n",
    "        return self.dt_arr, new_df, pq_arr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed61fed-0c02-4480-b615-d694459ca31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to run the testing data on the trees created through training,\n",
    "# Made a traversal method to create the dataframes/leaves that result in the \n",
    "# Test data being run through the trees, then for each leaf, I assign the predicted\n",
    "# Values that were found in training (the leaf_queue) \n",
    "\n",
    "def traverse_tree(data, dtree, num_learner, leaf_queue, leaf_list):\n",
    "    \n",
    "    root = dtree\n",
    "    if isLeaf(root):\n",
    "        # stop process of splitting     \n",
    "        data.loc[:,f'F{num_learner}'] = leaf_queue.get()\n",
    "        leaf_list.append(data)\n",
    "        if leaf_queue.empty():\n",
    "            return 0\n",
    "    else:\n",
    "        split_val = dtree.value[1]\n",
    "        split_feat = dtree.value[3] \n",
    "\n",
    "        left_data = data[data[split_feat] <= split_val]\n",
    "        right_data = data[data[split_feat] > split_val]\n",
    "\n",
    "        traverse_tree(left_data, dtree.left, num_learner, leaf_queue, leaf_list)\n",
    "        traverse_tree(right_data, dtree.right, num_learner, leaf_queue, leaf_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33cd5b35-1526-45aa-9e14-5616e5796887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code is used to create new \"batches\" (10 new learners/regression trees)\n",
    "# In order to maintain JN's memory limit, each new csv file saves only the most recent prediction (log odds), probability, and residuals\n",
    "# as these are the only \"features\" needed to build the next sequential learner \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# init_df = pd.read_csv('batch_9_train.csv') # CHANGE\n",
    "# excluding_cols = []\n",
    "# resid = 90 # CHANGE\n",
    "# for i in range(resid): \n",
    "#     excluding_cols.append(f'F{i}')\n",
    "#     excluding_cols.append(f'residual {i}')\n",
    "#     excluding_cols.append(f'pred. prob {i}')\n",
    "# train_df = init_df.loc[:, ~init_df.columns.isin(excluding_cols)]\n",
    "\n",
    "# def train_test_batch(df, r_num, batch_num):\n",
    "#     batch = GradientBoostingMachine(df, num_learners = 10, depth = 2, dt_arr = [], learning_rate = 0.05)\n",
    "#     train_pred = batch.train_next_batches(resid_num = r_num)\n",
    "#     dt_arr_len = len(train_pred[0])\n",
    "#     dt_arr = train_pred[0][0:dt_arr_len - 1]\n",
    "#     pq_arr = train_pred[2]\n",
    "    \n",
    "#     batch_df = train_pred[1]\n",
    "#     batch_df.to_csv(f'batch_{batch_num}_train.csv', index = False)\n",
    "\n",
    "\n",
    "#     X_test = pd.read_csv(f'batch_{batch_num - 1}_test.csv')\n",
    "#     i = 0 \n",
    "#     n_estimate = r_num\n",
    "#     for d in dt_arr:\n",
    "#         leaf_list = [] \n",
    "#         traverse_tree(X_test, d, n_estimate + 1, pq_arr[i], leaf_list)\n",
    "#         X_test = pd.concat(leaf_list)\n",
    "#         i += 1\n",
    "#         n_estimate += 1\n",
    "    \n",
    "#     X_test.to_csv(f'batch_{batch_num}_test.csv', index = False)\n",
    "# train_test_batch(train_df, resid, 10) # CHANGE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db3ad2f-8944-453a-8a2f-d5a65482b3bf",
   "metadata": {},
   "source": [
    "### GBM Implementation Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd55c08-0057-4efd-85a4-dc59bdd93eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results = pd.read_csv('batch_10_train.csv')\n",
    "train_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36aa714a-7040-4a77-8117-2d5bc6c27a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_arr = [] \n",
    "train_preds = train_results['pred. prob 100']\n",
    "def try_threshold(p):\n",
    "    for pred in train_preds:\n",
    "        if pred <= p: \n",
    "            pred_arr.append(0)\n",
    "        if pred > p:\n",
    "            pred_arr.append(1)\n",
    "    train_results['predicted target'] = pred_arr\n",
    "    \n",
    "    fpr_space = train_results[train_results['target'] == 0] \n",
    "    fps_pred = fpr_space[fpr_space['predicted target'] == 1]\n",
    "\n",
    "    tpr_space = train_results[train_results['target'] == 1] \n",
    "    tps_pred = tpr_space[tpr_space['predicted target'] == 0]\n",
    "    \n",
    "    fpr = fps_pred.shape[0] / fpr_space.shape[0]\n",
    "    tpr = tps_pred.shape[0] / tpr_space.shape[0]\n",
    "    return fpr, tpr\n",
    "\n",
    "\n",
    "unique_thresholds = train_results['pred. prob 100'].unique()\n",
    "fpr_tpr_arr = []\n",
    "for u in unique_thresholds:\n",
    "    pred_arr = []\n",
    "    train_preds = train_results['pred. prob 100']\n",
    "    fpr, tpr = try_threshold(u)\n",
    "    fpr_tpr_arr.append([u, fpr, tpr])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d379f4-a92b-4aff-86bc-8d44e744acd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_tpr_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e759119-9f96-41e2-8299-0d962b8a2eee",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fpr_tpr_arr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m opt_threshold = \u001b[43mfpr_tpr_arr\u001b[49m[\u001b[32m1\u001b[39m][\u001b[32m0\u001b[39m]\n\u001b[32m      2\u001b[39m opt_threshold\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# chose the threshold that minimized the fpr and maximized the tpr for the training data,\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# and used that as the threshold used to determine the class to assign each client in testing data\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'fpr_tpr_arr' is not defined"
     ]
    }
   ],
   "source": [
    "opt_threshold = fpr_tpr_arr[1][0]\n",
    "opt_threshold\n",
    "\n",
    "# chose the threshold that minimized the fpr and maximized the tpr for the training data,\n",
    "# and used that as the threshold used to determine the class to assign each client in testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314ba38e-b5d3-49c6-8219-a071d42f811d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = pd.read_csv('batch_10_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afd47f5-aa6e-494d-9c9c-261474596655",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_init = pd.read_csv('batch_1_train.csv')\n",
    "\n",
    "predys = []\n",
    "\n",
    "init_log_odds = get_init['F0'][0] #F0\n",
    "for i in range(1, 101): \n",
    "    init_log_odds = init_log_odds + 0.1*test_results[f'F{i}']\n",
    "\n",
    "\n",
    "for i in init_log_odds:\n",
    "    if i >= 1000:\n",
    "        predys.append(1)\n",
    "    else:\n",
    "        pred_prob = np.exp(i) / (np.exp(i) + 1)\n",
    "        predys.append(pred_prob)\n",
    "\n",
    "test_results['predicted probs'] = predys\n",
    "\n",
    "\n",
    "pred_arr = [] \n",
    "for p in predys:\n",
    "    if p <= opt_threshold: \n",
    "        pred_arr.append(0)\n",
    "    if p > opt_threshold:\n",
    "        pred_arr.append(1)\n",
    "\n",
    "test_results['GBM predictions'] = pred_arr\n",
    "test_results['Baseline predictions'] = 0\n",
    "\n",
    "gb_accuracy = 1 - ((np.sum(np.abs(test_results['actual target'] - pred_arr))) / 6000)\n",
    "baseline_accuracy = 1 - (np.sum(np.abs(test_results['actual target'] - 0)) / 6000)\n",
    "\n",
    "gb_accuracy, baseline_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137e4ae7-1766-4788-aed6-13cd897afd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# created these functions simply to refresh memory on model performance metrics, will use built in sklearn libs for comparison section\n",
    "def get_TPR(data, target, pred):\n",
    "    tp_space = data[data[target] == 1]     \n",
    "    tp_df = tp_space[tp_space[pred] == 1]\n",
    "\n",
    "    tp_num = tp_df.shape[0]\n",
    "    tp_tot = tp_space.shape[0]\n",
    "    tpr = tp_num / tp_tot\n",
    "    \n",
    "    return [tpr, tp_num, tp_tot ]\n",
    "\n",
    "def get_FPR(data, target, pred):\n",
    "    fp_space = data[data[target] == 0]     \n",
    "    fp_df = fp_space[fp_space[pred] == 1]\n",
    "\n",
    "    fp_num = fp_df.shape[0]\n",
    "    fp_tot = fp_space.shape[0]\n",
    "    fpr = fp_num / fp_tot\n",
    "    return [fpr, fp_num, fp_tot]\n",
    "    \n",
    "\n",
    "baseline_fpr = get_FPR(test_results, 'actual target', 'Baseline predictions')[0]\n",
    "baseline_tpr = get_TPR(test_results, 'actual target', 'Baseline predictions')[0]\n",
    "\n",
    "gb_fpr = get_FPR(test_results, 'actual target', 'GBM predictions')[0]\n",
    "gb_tpr = get_TPR(test_results, 'actual target', 'GBM predictions')[0]\n",
    "\n",
    "gb_tp_num = get_TPR(test_results, 'actual target', 'GBM predictions')[1] \n",
    "gb_prec_tot = gb_tp_num + get_FPR(test_results, 'actual target', 'GBM predictions')[1]\n",
    "baseline_precision =  0 # 0/0\n",
    "gb_precision = gb_tp_num / gb_prec_tot\n",
    "\n",
    "baseline_f1 = 0\n",
    "gb_f1 = (2*gb_precision*gb_tpr) / (gb_precision + gb_tpr)\n",
    "\n",
    "print(f'GBM accuracy: {gb_accuracy}, Baseline_accuracy: {baseline_accuracy}')\n",
    "print(f'GBM TPR: {gb_tpr}, Baseline TPR: {baseline_tpr}')\n",
    "print(f'GBM FPR: {gb_fpr}, Baseline FPR: {baseline_fpr}')\n",
    "print(f'GBM TNR: {1 - gb_fpr}, Baseline TNR: {1 - baseline_fpr}')\n",
    "print(f'GBM FNR: {1 - gb_tpr}, Baseline FNR: {1 - baseline_tpr}')\n",
    "print(f'GBM Precision: {gb_precision}, Baseline Precision: {baseline_precision}')\n",
    "\n",
    "print(f'GBM F1 Score: {gb_f1}, Baseline F1 Score: {baseline_f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e9639e-7125-40ce-ad3b-10cfa723a711",
   "metadata": {},
   "source": [
    "### GBM Implementation vs. Scikit-learn's GBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca7833d-73e9-4c69-9cee-ee8752ee92e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d010328-f9d2-469b-b517-34dee692f68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(test_results['actual target'], test_results['GBM predictions'], labels = [0,1])\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = [0, 1])\n",
    "disp.plot()\n",
    "plt.title('GBM Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdfcb40-5642-4d07-b0e9-3b63fb09917c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# workflow:\n",
    "\n",
    "# new batch test df has just previous residual, pred. prob, F{i-1} as well as OG features \n",
    "# test retains all columns to output final prediction by the GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a87c6c-3c35-437b-aa29-e5f8e0bdf00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cda69b0-4cbc-4575-b24c-42c4392f77f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pred_arr\n",
    "y_test = test_results['actual target'].to_numpy()\n",
    "\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a1ace6-6a7f-4d24-8ddd-1fa1a2ea36a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7713dd48-5cfb-4baa-bcdb-a8cb38a25bec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
