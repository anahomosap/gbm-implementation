{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30d6a011-f0d8-4ef5-a70a-f65d4e58eef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ucimlrepo\n",
      "  Using cached ucimlrepo-0.0.7-py3-none-any.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: pandas>=1.0.0 in /srv/conda/lib/python3.11/site-packages (from ucimlrepo) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2020.12.5 in /srv/conda/lib/python3.11/site-packages (from ucimlrepo) (2025.8.3)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /srv/conda/lib/python3.11/site-packages (from pandas>=1.0.0->ucimlrepo) (2.2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /srv/conda/lib/python3.11/site-packages (from pandas>=1.0.0->ucimlrepo) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /srv/conda/lib/python3.11/site-packages (from pandas>=1.0.0->ucimlrepo) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /srv/conda/lib/python3.11/site-packages (from pandas>=1.0.0->ucimlrepo) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /srv/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.17.0)\n",
      "Using cached ucimlrepo-0.0.7-py3-none-any.whl (8.0 kB)\n",
      "Installing collected packages: ucimlrepo\n",
      "Successfully installed ucimlrepo-0.0.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ucimlrepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9f15a39-b328-4d4a-ac90-73ab7fb25095",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import queue\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8e46540-7d0f-4a33-aaa9-e89ab723297e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to import dataset to python -- pasted from UCI ML repo \n",
    "\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "   \n",
    "default_of_credit_card_clients = fetch_ucirepo(id=350) \n",
    "\n",
    "default_cc = default_of_credit_card_clients.data.original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7631c22f-13b5-4b60-bdda-ef07d4652e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: One Hot Encoding qualitative features to be used for Regression Tree implementation\n",
    "def ohe_qual_feats(data):\n",
    "    qual_feat = ['X2', 'X3', 'X4']\n",
    "    for ql in qual_feat:\n",
    "        oh_test = data[[ql]]\n",
    "        ohe = OneHotEncoder(handle_unknown = 'ignore', sparse_output = False)\n",
    "        one_hot_encoded = ohe.fit_transform(oh_test)\n",
    "        one_hot_df = pd.DataFrame(one_hot_encoded, columns=ohe.get_feature_names_out([ql]))\n",
    "        data = pd.concat([data, one_hot_df], axis = 1)\n",
    "    data = data.drop(columns = ['X2', 'X3', 'X4'])\n",
    "    return data\n",
    "\n",
    "default_cc = ohe_qual_feats(default_cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4065766-48d2-44f7-94e3-8aee9b2b01e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating X and y tables from original table that is then split into a training and testing set\n",
    "\n",
    "X = default_cc.loc[:, default_cc.columns != 'Y'] \n",
    "y = default_cc.loc[:, 'Y'].to_frame(name = 'target')\n",
    "   \n",
    "default_of_credit_card_clients.data.original\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5cd0083d-c1e8-433e-b0d9-9609fc115702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating test_df which is the training data combined (so both the features and the target variable in one table)\n",
    "# This is to have the DTs to be created based on the true values (target) \n",
    "\n",
    "test_df = X_train\n",
    "test_df['target'] = y_train\n",
    "test_df = test_df.drop('ID', axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32bae034-22df-4c6e-8d0b-65e2ca4467e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for different points in a continuous feature, I wanted to find the boundary that minimizes the residuals\n",
    "# this method gathers 9 points (percentiles from the data)\n",
    "# I chose to do this over gradient descent simply because it is lest costly and would bring minimal change to the optimal boundary \n",
    "\n",
    "def minimize_stats(data, feat, target):\n",
    "    quant_df = data[[feat]] \n",
    "    qs = np.array(quant_df.quantile([0.1, 0.25, 0.3, 0.4, 0.5, 0.6, 0.75, 0.8, 0.9])[feat])\n",
    "    return qs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f19ce314-3ad4-40ad-bec4-6f960c4ce657",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# need this particular formula to convert leaf outputs to predicted probabilities\n",
    "\n",
    "def lodds_to_prob(l_odds):\n",
    "    pred_prob =  np.exp(l_odds) / (1 + np.exp(l_odds))\n",
    "    return pred_prob\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0509d7d3-2247-4cd0-adf3-36cd3b834e99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(11409.97122969299),\n",
       " 0,\n",
       " np.float64(-4.760636329592671e-13),\n",
       " 'X6',\n",
       "            X1  X5  X6  X7  X8  X9  X10  X11    X12    X13  ...  X3_2  X3_3  \\\n",
       " 25295   60000  24   0   0   0   0    0    0  50840  49592  ...   1.0   0.0   \n",
       " 23909   80000  26  -1   3   2  -1    2    2    495    330  ...   1.0   0.0   \n",
       " 25048  100000  54   0   0   0   0    5    4  37082  46041  ...   0.0   1.0   \n",
       " 17022   80000  23   0   0   0   0    0    0   6805   8449  ...   1.0   0.0   \n",
       " 5917   200000  46  -1  -1  -1  -1   -2   -2   1207   5590  ...   0.0   1.0   \n",
       " ...       ...  ..  ..  ..  ..  ..  ...  ...    ...    ...  ...   ...   ...   \n",
       " 12895   50000  25   0   0   0   0    0    0  50485  50397  ...   1.0   0.0   \n",
       " 28192  170000  34  -2  -2  -1   0    0   -2   1088   1088  ...   0.0   0.0   \n",
       " 6012   360000  47  -2  -2  -2  -2   -2   -2   1458     -4  ...   0.0   1.0   \n",
       " 6558   180000  30  -1  -1  -1  -1   -1   -2   1730   6792  ...   0.0   1.0   \n",
       " 23499  140000  33   1  -1  -1  -1   -1   -1    -23   5742  ...   1.0   0.0   \n",
       " \n",
       "        X3_4  X3_5  X3_6  X4_0  X4_1  X4_2  X4_3  target  \n",
       " 25295   0.0   0.0   0.0   0.0   0.0   1.0   0.0       0  \n",
       " 23909   0.0   0.0   0.0   0.0   1.0   0.0   0.0       0  \n",
       " 25048   0.0   0.0   0.0   0.0   1.0   0.0   0.0       1  \n",
       " 17022   0.0   0.0   0.0   0.0   0.0   1.0   0.0       0  \n",
       " 5917    0.0   0.0   0.0   0.0   1.0   0.0   0.0       1  \n",
       " ...     ...   ...   ...   ...   ...   ...   ...     ...  \n",
       " 12895   0.0   0.0   0.0   0.0   0.0   1.0   0.0       0  \n",
       " 28192   0.0   0.0   0.0   0.0   0.0   1.0   0.0       0  \n",
       " 6012    0.0   0.0   0.0   0.0   1.0   0.0   0.0       1  \n",
       " 6558    0.0   0.0   0.0   0.0   0.0   1.0   0.0       0  \n",
       " 23499   0.0   0.0   0.0   0.0   1.0   0.0   0.0       0  \n",
       " \n",
       " [24000 rows x 34 columns])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gets the residual reduction from a particular split of a feature\n",
    "# weights False Negatives to be 4 times \"more\" than False Positives\n",
    "# encourages residual trees to predict true positives correctly \n",
    "\n",
    "def get_rss(data, feat, target, bounds):\n",
    "    rss_arr = np.empty(len(bounds))\n",
    "    grad_arr = np.empty(len(bounds))\n",
    "    \n",
    "    for i, b in enumerate(bounds):\n",
    "        dat_l = data[data[feat] > b]\n",
    "        dat_u = data[data[feat] <= b]\n",
    "\n",
    "        mean_l = np.mean(dat_l[target])\n",
    "        mean_u = np.mean(dat_u[target])\n",
    "\n",
    "        rss_l = 0\n",
    "        rss_u = 0\n",
    "        \n",
    "        true_val_u = dat_u['target'].to_numpy()\n",
    "        true_val_l = dat_l['target'].to_numpy()\n",
    "\n",
    "        target_u = dat_u[target].to_numpy()\n",
    "        target_l = dat_l[target].to_numpy()\n",
    "        \n",
    "        for j, u in enumerate(true_val_u):\n",
    "            if u == 1:\n",
    "                rss_u += 4*((target_u[j] - mean_u)**2)\n",
    "            else:    \n",
    "                rss_u += ((target_u[j] - mean_u)**2)\n",
    "                \n",
    "        for k, l in enumerate(true_val_l):\n",
    "            if l == 1:\n",
    "                rss_l += 4*((target_l[k] - mean_l)**2)\n",
    "            else: \n",
    "                rss_l += ((target_l[k] - mean_l)**2)\n",
    "        \n",
    "        \n",
    "        \n",
    "        grad_l  = np.sum(-2*(dat_l[target]  - mean_l)) \n",
    "        grad_u = np.sum(-2*(dat_u[target] - mean_u))\n",
    "        \n",
    "        rss_arr[i] = rss_l + rss_u\n",
    "        grad_arr[i] = grad_l + grad_u\n",
    "\n",
    "    rss_min = np.min(rss_arr)\n",
    "    min_ind = np.argmin(rss_arr)\n",
    "    \n",
    "    # returns minimum rss, the optimal split, gradient, the feature name, and the orginal dataframe\n",
    "    return rss_min, bounds[min_ind], grad_arr[min_ind], feat, data\n",
    "\n",
    "get_rss(test_df, 'X6', 'target', [0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc8a50f0-c436-4d07-87ab-15c7cdcdbfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each point/node in tree, find the feature that minimizes the residual sum of squares\n",
    "# this will select the feature/split for each DT node\n",
    "# inputs are the data at that particular node, and target represents the feature that the DT is trying to predict\n",
    "\n",
    "def minimal_rss_quant(data, target):\n",
    "    feats = ['X1', 'X5', 'X6', 'X7', 'X8', 'X9', 'X10', 'X11', 'X12', 'X13', 'X14',\n",
    "       'X15', 'X16', 'X17', 'X18', 'X19', 'X20', 'X21', 'X22', 'X23', 'X2_1',\n",
    "       'X2_2', 'X3_0', 'X3_1', 'X3_2', 'X3_3', 'X3_4', 'X3_5', 'X3_6', 'X4_0',\n",
    "       'X4_1', 'X4_2', 'X4_3', target]\n",
    "    dater = []\n",
    "    min_rss = np.inf\n",
    "    min_stats = None\n",
    "    \n",
    "    for f in feats:\n",
    "        if f != target:\n",
    "            bounds = minimize_stats(data, f, target)\n",
    "            stats = get_rss(data, f, target, bounds)\n",
    "            rss = stats[0]\n",
    "\n",
    "            if rss < min_rss:\n",
    "                min_rss = rss\n",
    "                min_stats = stats\n",
    "    return min_stats\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4903c6c-8294-4f02-90f4-9db9a800b8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created DTNode to represent each node of the DTs created, \n",
    "# DecisionTreeRegressor class helps traverse the tree, create the tree, and \n",
    "# identify which nodes are considered leaves (will be useful later) \n",
    "\n",
    "class DTNode: \n",
    "\n",
    "    def __init__(self):\n",
    "        self.value = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "\n",
    "\n",
    "class DecisionTreeRegressor:\n",
    "\n",
    "    def __init__(self, data, depth, target):\n",
    "        self.depth = depth  \n",
    "        self.data = data\n",
    "        self.target = target\n",
    "\n",
    "    def create_tree(self):\n",
    "        self.dtree = build_tree(self.data, self.depth, self.target)\n",
    "        return self.dtree\n",
    "\n",
    "    def isLeaf(dtnode):\n",
    "        return ((dtnode.right is None) and (dtnode.left is None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4efdd9d8-14bf-46d3-acb1-89007cafc119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build_tree takes in the dataframe at this point (whatever split, the depth of the tree - as indicated when the DTRegressor class \n",
    "# is instantiated; uses recursion to continually split the new data split the most optimally, and continues until the indicated depth is reached\n",
    "\n",
    "def build_tree(data, depth, target):\n",
    "    if data.shape[0] > 0:\n",
    "        root = DTNode()\n",
    "        root.value = minimal_rss_quant(data, target)\n",
    "    else: \n",
    "        return 0 \n",
    " \n",
    "    if depth == 0:\n",
    "        return root  \n",
    "    else:\n",
    "        children_data = create_children(data, root)\n",
    "        \n",
    "        root.left = DTNode()\n",
    "        root.right = DTNode() \n",
    "\n",
    "        depth = depth - 1\n",
    "        root.left = build_tree(children_data[0], depth, target)\n",
    "        root.right = build_tree(children_data[1], depth, target)\n",
    "    return root\n",
    "\n",
    "# create_children is a helper function for build_tree in order to create new data frames upon splitting the data\n",
    "# e.g. if the feature to split on is X6, and the value is 1 then create_children takes the data and splits it into the following:\n",
    "# left node/dataframe : clients that have X6 <= 1 , right node/dataframe : clients that have X6 > 1\n",
    "\n",
    "def create_children(data, node):\n",
    "    feat_name = node.value[3]\n",
    "    pivot_val = node.value[1]\n",
    "    \n",
    "    left_data = data[data[feat_name] <= pivot_val]\n",
    "    right_data = data[data[feat_name] > pivot_val]\n",
    "\n",
    "    return left_data, right_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cdd3a1e6-aba1-4713-a2c6-708180bbd11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_new_val creates the new predictions for each leaf given the previous\n",
    "\n",
    "def get_new_val(data, resid, guess_col):\n",
    "    probs = lodds_to_prob(data[guess_col])\n",
    "    new_pred = np.sum(data[resid]) / np.sum(probs * (1 - probs))\n",
    "    return new_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "efcfe721-4995-44e5-8f2f-2bef2237b05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# leaf_arr[1]\n",
    "\n",
    "\n",
    "\n",
    "# for each decision tree,\n",
    "# find what value the output will predict\n",
    "# and then sum up those values F0 + v*sum blah blah (35:06)\n",
    "\n",
    "# Testing...\n",
    "\n",
    "# for each tree, store every node's split (less than + greater than/equal)\n",
    "# Run test data through the tree to get leaves, (in testing... store leaf values to be able to assign them to test)\n",
    "# Then repeat this with the next tree, with the predicted residuals/log(odds) from ^^^\n",
    "# have 2nd tree data stored, and assign leaves and repeat until dones with all trees\n",
    "# each observation shoudl now have some probability that it is 1 or 0, find probability threshold,, maybe use ROC or AUC to determine best threshold??\n",
    "# then assign final predicted values 0, 1\n",
    "# do model evaluations (confusion matrix, precision, recall, accuracy, F1 score, ROC curve\n",
    "\n",
    "#get_F0(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9072033-e59d-4f32-bc05-d0e8b6c17678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get initial prediction \n",
    "def get_F0(data): \n",
    "    p_1 = data[data['target'] == 1].shape[0] / len(data['target'])\n",
    "    odds = p_1 / (1 - p_1)\n",
    "    log_odds = np.log(odds)\n",
    "    return log_odds\n",
    "\n",
    "def lodds_to_prob(l_odds):\n",
    "    pred_prob =  np.exp(l_odds) / (1 + np.exp(l_odds))\n",
    "    return pred_prob\n",
    "\n",
    "class GBMInitializer:\n",
    "    def __init__(self, data, depth):\n",
    "        self.data = data\n",
    "        self.depth = depth\n",
    "        \n",
    "    def initialize_data(self):\n",
    "        lodds = get_F0(self.data)\n",
    "        prob_0 = lodds_to_prob(lodds)\n",
    "        self.data['F0'] = lodds\n",
    "        self.data['pred. prob 0'] = prob_0\n",
    "        self.data['residual 0'] = self.data['target'] - prob_0\n",
    "        return 0\n",
    "\n",
    "    def create_regression_tree(self, target):\n",
    "        fake_tree = DecisionTreeRegressor(self.data, self.depth, target)\n",
    "        doo = fake_tree.create_tree()\n",
    "        return doo\n",
    "\n",
    "    def create_model():\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "779a8f0b-b1b1-4577-89c8-08535c938d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isLeaf(dtnode):\n",
    "    return ((dtnode.right is None) and (dtnode.left is None))\n",
    "    \n",
    "def returnLeaves(root):\n",
    "    leaf_arr = [] \n",
    "    def getLeaves(root_node): \n",
    "        if isLeaf(root_node):\n",
    "            leaf_arr.append([root_node.value[4]])\n",
    "        else:\n",
    "            getLeaves(root_node.left)\n",
    "            getLeaves(root_node.right)\n",
    "    getLeaves(root)\n",
    "    return leaf_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a4c3d09-d77c-4fa8-9079-c331a557d126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>X11</th>\n",
       "      <th>X12</th>\n",
       "      <th>X13</th>\n",
       "      <th>...</th>\n",
       "      <th>X3_2</th>\n",
       "      <th>X3_3</th>\n",
       "      <th>X3_4</th>\n",
       "      <th>X3_5</th>\n",
       "      <th>X3_6</th>\n",
       "      <th>X4_0</th>\n",
       "      <th>X4_1</th>\n",
       "      <th>X4_2</th>\n",
       "      <th>X4_3</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25295</th>\n",
       "      <td>60000</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50840</td>\n",
       "      <td>49592</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23909</th>\n",
       "      <td>80000</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>495</td>\n",
       "      <td>330</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25048</th>\n",
       "      <td>100000</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>37082</td>\n",
       "      <td>46041</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17022</th>\n",
       "      <td>80000</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6805</td>\n",
       "      <td>8449</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5917</th>\n",
       "      <td>200000</td>\n",
       "      <td>46</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1207</td>\n",
       "      <td>5590</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12895</th>\n",
       "      <td>50000</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50485</td>\n",
       "      <td>50397</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28192</th>\n",
       "      <td>170000</td>\n",
       "      <td>34</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>1088</td>\n",
       "      <td>1088</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6012</th>\n",
       "      <td>360000</td>\n",
       "      <td>47</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1458</td>\n",
       "      <td>-4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6558</th>\n",
       "      <td>180000</td>\n",
       "      <td>30</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>1730</td>\n",
       "      <td>6792</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23499</th>\n",
       "      <td>140000</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-23</td>\n",
       "      <td>5742</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24000 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           X1  X5  X6  X7  X8  X9  X10  X11    X12    X13  ...  X3_2  X3_3  \\\n",
       "25295   60000  24   0   0   0   0    0    0  50840  49592  ...   1.0   0.0   \n",
       "23909   80000  26  -1   3   2  -1    2    2    495    330  ...   1.0   0.0   \n",
       "25048  100000  54   0   0   0   0    5    4  37082  46041  ...   0.0   1.0   \n",
       "17022   80000  23   0   0   0   0    0    0   6805   8449  ...   1.0   0.0   \n",
       "5917   200000  46  -1  -1  -1  -1   -2   -2   1207   5590  ...   0.0   1.0   \n",
       "...       ...  ..  ..  ..  ..  ..  ...  ...    ...    ...  ...   ...   ...   \n",
       "12895   50000  25   0   0   0   0    0    0  50485  50397  ...   1.0   0.0   \n",
       "28192  170000  34  -2  -2  -1   0    0   -2   1088   1088  ...   0.0   0.0   \n",
       "6012   360000  47  -2  -2  -2  -2   -2   -2   1458     -4  ...   0.0   1.0   \n",
       "6558   180000  30  -1  -1  -1  -1   -1   -2   1730   6792  ...   0.0   1.0   \n",
       "23499  140000  33   1  -1  -1  -1   -1   -1    -23   5742  ...   1.0   0.0   \n",
       "\n",
       "       X3_4  X3_5  X3_6  X4_0  X4_1  X4_2  X4_3  target  \n",
       "25295   0.0   0.0   0.0   0.0   0.0   1.0   0.0       0  \n",
       "23909   0.0   0.0   0.0   0.0   1.0   0.0   0.0       0  \n",
       "25048   0.0   0.0   0.0   0.0   1.0   0.0   0.0       1  \n",
       "17022   0.0   0.0   0.0   0.0   0.0   1.0   0.0       0  \n",
       "5917    0.0   0.0   0.0   0.0   1.0   0.0   0.0       1  \n",
       "...     ...   ...   ...   ...   ...   ...   ...     ...  \n",
       "12895   0.0   0.0   0.0   0.0   0.0   1.0   0.0       0  \n",
       "28192   0.0   0.0   0.0   0.0   0.0   1.0   0.0       0  \n",
       "6012    0.0   0.0   0.0   0.0   1.0   0.0   0.0       1  \n",
       "6558    0.0   0.0   0.0   0.0   0.0   1.0   0.0       0  \n",
       "23499   0.0   0.0   0.0   0.0   1.0   0.0   0.0       0  \n",
       "\n",
       "[24000 rows x 34 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb28e337-ec05-4bef-8c0a-a3813667c7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_val(data, resid, probs):\n",
    "    new_pred = np.sum(data[resid]) / np.sum(data[probs] * (1 - data[probs]))\n",
    "    return new_pred\n",
    "\n",
    "\n",
    "class GradientBoostingMachine:\n",
    "    def __init__(self, data, num_learners, depth, dt_arr, learning_rate):\n",
    "        self.data = data\n",
    "        self.num_learners = num_learners\n",
    "        self.depth = depth\n",
    "        self.dt_arr = dt_arr\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def train(self): \n",
    "        gbm = GBMInitializer(self.data, self.depth)\n",
    "        gbm.initialize_data()\n",
    "        dt_tree = gbm.create_regression_tree('residual 0')\n",
    "        leaves = returnLeaves(dt_tree)\n",
    "\n",
    "        self.dt_arr.append(dt_tree)\n",
    "        pq_arr = []\n",
    "        for i in range(1, self.num_learners + 1):\n",
    "            new_leaves =[]\n",
    "            pq = queue.Queue()\n",
    "            for l in leaves:\n",
    "                l = l.copy()\n",
    "                new_pred = get_new_val(l[0], f'residual {i - 1}', f'pred. prob {i - 1}')\n",
    "                #l.append(new_pred) # new line.. see if this works\n",
    "                l[0].loc[:,f'F{i}'] = new_pred\n",
    "                new_leaves.append(l[0])\n",
    "                pq.put(new_pred)\n",
    "            \n",
    "            new_df = pd.concat(new_leaves, axis = 0)\n",
    "            odds = new_df[f'F{i - 1}'] + self.learning_rate*new_df[f'F{i}']\n",
    "            new_df[f'pred. prob {i}'] = lodds_to_prob(odds)\n",
    "            new_df[f'residual {i}'] = new_df['target'] - new_df[f'pred. prob {i}']\n",
    "\n",
    "            pq_arr.append(pq)\n",
    "            ore = GBMInitializer(new_df, 2)\n",
    "            dt_2 = ore.create_regression_tree(f'residual {i}')\n",
    "            self.dt_arr.append(dt_2)\n",
    "            leaves = returnLeaves(dt_2)\n",
    "        return self.dt_arr, new_df, pq_arr\n",
    "\n",
    "    \n",
    "    def train_next_batches(self, resid_num):\n",
    "        gbm = GBMInitializer(self.data, self.depth)\n",
    "        dt_tree2 = gbm.create_regression_tree(f'residual {resid_num}')\n",
    "        leaves = returnLeaves(dt_tree2)\n",
    "        \n",
    "        self.dt_arr.append(dt_tree2)\n",
    "        pq_arr2 = []\n",
    "        for i in range(resid_num + 1, resid_num + self.num_learners + 1):\n",
    "            new_leaves =[]\n",
    "            pq = queue.Queue()\n",
    "            for l in leaves:\n",
    "                l = l.copy()\n",
    "                new_pred = get_new_val(l[0], f'residual {i - 1}', f'pred. prob {i - 1}')\n",
    "                #l.append(new_pred) # new line.. see if this works\n",
    "                l[0].loc[:,f'F{i}'] = new_pred\n",
    "                new_leaves.append(l[0])\n",
    "                pq.put(new_pred)\n",
    "            \n",
    "            new_df = pd.concat(new_leaves, axis = 0)\n",
    "            odds = new_df[f'F{i - 1}'] + self.learning_rate*new_df[f'F{i}']\n",
    "            new_df[f'pred. prob {i}'] = lodds_to_prob(odds)\n",
    "            new_df[f'residual {i}'] = new_df['target'] - new_df[f'pred. prob {i}']\n",
    "\n",
    "            pq_arr2.append(pq)\n",
    "            ore = GBMInitializer(new_df, 2)\n",
    "            dt_2 = ore.create_regression_tree(f'residual {i}')\n",
    "            self.dt_arr.append(dt_2)\n",
    "            leaves = returnLeaves(dt_2)\n",
    "        return self.dt_arr, new_df, pq_arr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aed61fed-0c02-4480-b615-d694459ca31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to run the testing data on the trees created through training,\n",
    "# Made a traversal method to create the dataframes/leaves that result in the \n",
    "# Test data being run through the trees, then for each leaf, I assign the predicted\n",
    "# Values that were found in training (the leaf_queue) \n",
    "\n",
    "def traverse_tree(data, dtree, num_learner, leaf_queue, leaf_list):\n",
    "    \n",
    "    root = dtree\n",
    "    if isLeaf(root):\n",
    "        # stop process of splitting     \n",
    "        data.loc[:,f'F{num_learner}'] = leaf_queue.get()\n",
    "        leaf_list.append(data)\n",
    "        if leaf_queue.empty():\n",
    "            return 0\n",
    "    else:\n",
    "        split_val = dtree.value[1]\n",
    "        split_feat = dtree.value[3] \n",
    "\n",
    "        left_data = data[data[split_feat] <= split_val]\n",
    "        right_data = data[data[split_feat] > split_val]\n",
    "\n",
    "        traverse_tree(left_data, dtree.left, num_learner, leaf_queue, leaf_list)\n",
    "        traverse_tree(right_data, dtree.right, num_learner, leaf_queue, leaf_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33cd5b35-1526-45aa-9e14-5616e5796887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code is used to create new \"batches\" (10 new learners/regression trees)\n",
    "# In order to maintain JN's memory limit, each new csv file saves only the most recent prediction (log odds), probability, and residuals\n",
    "# as these are the only \"features\" needed to build the next sequential learner \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# init_df = pd.read_csv('batch_9_train.csv') # CHANGE\n",
    "# excluding_cols = []\n",
    "# resid = 90 # CHANGE\n",
    "# for i in range(resid): \n",
    "#     excluding_cols.append(f'F{i}')\n",
    "#     excluding_cols.append(f'residual {i}')\n",
    "#     excluding_cols.append(f'pred. prob {i}')\n",
    "# train_df = init_df.loc[:, ~init_df.columns.isin(excluding_cols)]\n",
    "\n",
    "# def train_test_batch(df, r_num, batch_num):\n",
    "#     batch = GradientBoostingMachine(df, num_learners = 10, depth = 2, dt_arr = [], learning_rate = 0.05)\n",
    "#     train_pred = batch.train_next_batches(resid_num = r_num)\n",
    "#     dt_arr_len = len(train_pred[0])\n",
    "#     dt_arr = train_pred[0][0:dt_arr_len - 1]\n",
    "#     pq_arr = train_pred[2]\n",
    "    \n",
    "#     batch_df = train_pred[1]\n",
    "#     batch_df.to_csv(f'batch_{batch_num}_train.csv', index = False)\n",
    "\n",
    "\n",
    "#     X_test = pd.read_csv(f'batch_{batch_num - 1}_test.csv')\n",
    "#     i = 0 \n",
    "#     n_estimate = r_num\n",
    "#     for d in dt_arr:\n",
    "#         leaf_list = [] \n",
    "#         traverse_tree(X_test, d, n_estimate + 1, pq_arr[i], leaf_list)\n",
    "#         X_test = pd.concat(leaf_list)\n",
    "#         i += 1\n",
    "#         n_estimate += 1\n",
    "    \n",
    "#     X_test.to_csv(f'batch_{batch_num}_test.csv', index = False)\n",
    "# train_test_batch(train_df, resid, 10) # CHANGE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db3ad2f-8944-453a-8a2f-d5a65482b3bf",
   "metadata": {},
   "source": [
    "### GBM Implementation Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d9643e1b-6681-4c41-aae6-c0193c13b023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/classification_project/final_project\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2bd55c08-0057-4efd-85a4-dc59bdd93eb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>X11</th>\n",
       "      <th>X12</th>\n",
       "      <th>X13</th>\n",
       "      <th>...</th>\n",
       "      <th>residual 97</th>\n",
       "      <th>F98</th>\n",
       "      <th>pred. prob 98</th>\n",
       "      <th>residual 98</th>\n",
       "      <th>F99</th>\n",
       "      <th>pred. prob 99</th>\n",
       "      <th>residual 99</th>\n",
       "      <th>F100</th>\n",
       "      <th>pred. prob 100</th>\n",
       "      <th>residual 100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>80000</td>\n",
       "      <td>31</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.376367</td>\n",
       "      <td>-0.853860</td>\n",
       "      <td>0.305534</td>\n",
       "      <td>-0.305534</td>\n",
       "      <td>0.025581</td>\n",
       "      <td>0.298892</td>\n",
       "      <td>-0.298892</td>\n",
       "      <td>-0.466154</td>\n",
       "      <td>0.500568</td>\n",
       "      <td>-0.500568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>320000</td>\n",
       "      <td>32</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.623633</td>\n",
       "      <td>-0.853860</td>\n",
       "      <td>0.305534</td>\n",
       "      <td>0.694466</td>\n",
       "      <td>0.025581</td>\n",
       "      <td>0.298892</td>\n",
       "      <td>0.701108</td>\n",
       "      <td>-0.466154</td>\n",
       "      <td>0.500568</td>\n",
       "      <td>0.499432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100000</td>\n",
       "      <td>28</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.376367</td>\n",
       "      <td>-0.853860</td>\n",
       "      <td>0.305534</td>\n",
       "      <td>-0.305534</td>\n",
       "      <td>0.025581</td>\n",
       "      <td>0.298892</td>\n",
       "      <td>-0.298892</td>\n",
       "      <td>-0.466154</td>\n",
       "      <td>0.500568</td>\n",
       "      <td>-0.500568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>310000</td>\n",
       "      <td>43</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.376367</td>\n",
       "      <td>-0.853860</td>\n",
       "      <td>0.305534</td>\n",
       "      <td>-0.305534</td>\n",
       "      <td>0.025581</td>\n",
       "      <td>0.298892</td>\n",
       "      <td>-0.298892</td>\n",
       "      <td>-0.466154</td>\n",
       "      <td>0.500568</td>\n",
       "      <td>-0.500568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>290000</td>\n",
       "      <td>44</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.376367</td>\n",
       "      <td>-0.853860</td>\n",
       "      <td>0.305534</td>\n",
       "      <td>-0.305534</td>\n",
       "      <td>0.025581</td>\n",
       "      <td>0.298892</td>\n",
       "      <td>-0.298892</td>\n",
       "      <td>-0.466154</td>\n",
       "      <td>0.500568</td>\n",
       "      <td>-0.500568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23995</th>\n",
       "      <td>150000</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>143947</td>\n",
       "      <td>147394</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.236609</td>\n",
       "      <td>0.299051</td>\n",
       "      <td>0.476765</td>\n",
       "      <td>-0.476765</td>\n",
       "      <td>-0.910598</td>\n",
       "      <td>0.563043</td>\n",
       "      <td>-0.563043</td>\n",
       "      <td>-1.166168</td>\n",
       "      <td>0.275098</td>\n",
       "      <td>-0.275098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23996</th>\n",
       "      <td>150000</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>156252</td>\n",
       "      <td>121999</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.236609</td>\n",
       "      <td>0.299051</td>\n",
       "      <td>0.476765</td>\n",
       "      <td>-0.476765</td>\n",
       "      <td>-0.910598</td>\n",
       "      <td>0.563043</td>\n",
       "      <td>-0.563043</td>\n",
       "      <td>-1.166168</td>\n",
       "      <td>0.275098</td>\n",
       "      <td>-0.275098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23997</th>\n",
       "      <td>130000</td>\n",
       "      <td>46</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>79705</td>\n",
       "      <td>59716</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.236609</td>\n",
       "      <td>0.299051</td>\n",
       "      <td>0.476765</td>\n",
       "      <td>-0.476765</td>\n",
       "      <td>-0.910598</td>\n",
       "      <td>0.563043</td>\n",
       "      <td>-0.563043</td>\n",
       "      <td>-1.166168</td>\n",
       "      <td>0.275098</td>\n",
       "      <td>-0.275098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23998</th>\n",
       "      <td>220000</td>\n",
       "      <td>37</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>189976</td>\n",
       "      <td>189665</td>\n",
       "      <td>...</td>\n",
       "      <td>0.763391</td>\n",
       "      <td>0.299051</td>\n",
       "      <td>0.476765</td>\n",
       "      <td>0.523235</td>\n",
       "      <td>-0.910598</td>\n",
       "      <td>0.563043</td>\n",
       "      <td>0.436957</td>\n",
       "      <td>-1.166168</td>\n",
       "      <td>0.275098</td>\n",
       "      <td>0.724902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23999</th>\n",
       "      <td>240000</td>\n",
       "      <td>41</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>239633</td>\n",
       "      <td>242710</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.236609</td>\n",
       "      <td>0.299051</td>\n",
       "      <td>0.476765</td>\n",
       "      <td>-0.476765</td>\n",
       "      <td>-0.293104</td>\n",
       "      <td>0.570624</td>\n",
       "      <td>-0.570624</td>\n",
       "      <td>-1.166168</td>\n",
       "      <td>0.413040</td>\n",
       "      <td>-0.413040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24000 rows × 67 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           X1  X5  X6  X7  X8  X9  X10  X11     X12     X13  ...  residual 97  \\\n",
       "0       80000  31  -2  -2  -2  -2   -2   -2       0       0  ...    -0.376367   \n",
       "1      320000  32  -2  -2  -2  -2   -2   -2      -1      -1  ...     0.623633   \n",
       "2      100000  28  -2  -2  -2  -2   -1   -1       0       0  ...    -0.376367   \n",
       "3      310000  43  -2  -2  -2  -2   -2   -2       0       0  ...    -0.376367   \n",
       "4      290000  44  -2  -2  -2  -2   -2   -2       0       0  ...    -0.376367   \n",
       "...       ...  ..  ..  ..  ..  ..  ...  ...     ...     ...  ...          ...   \n",
       "23995  150000  42   2   0   0   0    0    0  143947  147394  ...    -0.236609   \n",
       "23996  150000  25   2   0   0   0    0    0  156252  121999  ...    -0.236609   \n",
       "23997  130000  46   2   0   0   0    0    0   79705   59716  ...    -0.236609   \n",
       "23998  220000  37   2   2   2   0    0    0  189976  189665  ...     0.763391   \n",
       "23999  240000  41   2   2   2   2    2    2  239633  242710  ...    -0.236609   \n",
       "\n",
       "            F98  pred. prob 98  residual 98       F99  pred. prob 99  \\\n",
       "0     -0.853860       0.305534    -0.305534  0.025581       0.298892   \n",
       "1     -0.853860       0.305534     0.694466  0.025581       0.298892   \n",
       "2     -0.853860       0.305534    -0.305534  0.025581       0.298892   \n",
       "3     -0.853860       0.305534    -0.305534  0.025581       0.298892   \n",
       "4     -0.853860       0.305534    -0.305534  0.025581       0.298892   \n",
       "...         ...            ...          ...       ...            ...   \n",
       "23995  0.299051       0.476765    -0.476765 -0.910598       0.563043   \n",
       "23996  0.299051       0.476765    -0.476765 -0.910598       0.563043   \n",
       "23997  0.299051       0.476765    -0.476765 -0.910598       0.563043   \n",
       "23998  0.299051       0.476765     0.523235 -0.910598       0.563043   \n",
       "23999  0.299051       0.476765    -0.476765 -0.293104       0.570624   \n",
       "\n",
       "       residual 99      F100  pred. prob 100  residual 100  \n",
       "0        -0.298892 -0.466154        0.500568     -0.500568  \n",
       "1         0.701108 -0.466154        0.500568      0.499432  \n",
       "2        -0.298892 -0.466154        0.500568     -0.500568  \n",
       "3        -0.298892 -0.466154        0.500568     -0.500568  \n",
       "4        -0.298892 -0.466154        0.500568     -0.500568  \n",
       "...            ...       ...             ...           ...  \n",
       "23995    -0.563043 -1.166168        0.275098     -0.275098  \n",
       "23996    -0.563043 -1.166168        0.275098     -0.275098  \n",
       "23997    -0.563043 -1.166168        0.275098     -0.275098  \n",
       "23998     0.436957 -1.166168        0.275098      0.724902  \n",
       "23999    -0.570624 -1.166168        0.413040     -0.413040  \n",
       "\n",
       "[24000 rows x 67 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_results = pd.read_csv('gbm-implementation/batch_10_train.csv')\n",
    "train_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "36aa714a-7040-4a77-8117-2d5bc6c27a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_arr = [] \n",
    "train_preds = train_results['pred. prob 100']\n",
    "def try_threshold(p):\n",
    "    for pred in train_preds:\n",
    "        if pred <= p: \n",
    "            pred_arr.append(0)\n",
    "        if pred > p:\n",
    "            pred_arr.append(1)\n",
    "    train_results['predicted target'] = pred_arr\n",
    "    \n",
    "    fpr_space = train_results[train_results['target'] == 0] \n",
    "    fps_pred = fpr_space[fpr_space['predicted target'] == 1]\n",
    "\n",
    "    tpr_space = train_results[train_results['target'] == 1] \n",
    "    tps_pred = tpr_space[tpr_space['predicted target'] == 0]\n",
    "    \n",
    "    fpr = fps_pred.shape[0] / fpr_space.shape[0]\n",
    "    tpr = tps_pred.shape[0] / tpr_space.shape[0]\n",
    "    return fpr, tpr\n",
    "\n",
    "\n",
    "unique_thresholds = train_results['pred. prob 100'].unique()\n",
    "fpr_tpr_arr = []\n",
    "for u in unique_thresholds:\n",
    "    pred_arr = []\n",
    "    train_preds = train_results['pred. prob 100']\n",
    "    fpr, tpr = try_threshold(u)\n",
    "    fpr_tpr_arr.append([u, fpr, tpr])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "04d379f4-a92b-4aff-86bc-8d44e744acd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[np.float64(0.5005682443626555), 0.0, 1.0],\n",
       " [np.float64(0.3619798957544952), 0.10316693505099302, 0.6577281191806331],\n",
       " [np.float64(0.28213291519667), 0.23687600644122384, 0.3972067039106145],\n",
       " [np.float64(0.4215504005499201), 0.06011808910359635, 0.8175046554934823],\n",
       " [np.float64(0.2781720814655709), 0.33263553408480945, 0.3191806331471136],\n",
       " [np.float64(0.4167686208021049), 0.06333870101986044, 0.813780260707635],\n",
       " [np.float64(0.3744625938194448), 0.09382716049382717, 0.746927374301676],\n",
       " [np.float64(0.2931271694420759), 0.2170155662909286, 0.5312849162011173],\n",
       " [np.float64(0.4346877760393332), 0.05067096081588835, 0.9195530726256983],\n",
       " [np.float64(0.353936155359063), 0.21690821256038648, 0.5316573556797021],\n",
       " [np.float64(0.2750984523396937), 0.9994632313472893, 0.0005586592178770949],\n",
       " [np.float64(0.4130398773359012), 0.09377348362855609, 0.746927374301676]]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpr_tpr_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3e759119-9f96-41e2-8299-0d962b8a2eee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.3619798957544952)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_threshold = fpr_tpr_arr[1][0]\n",
    "opt_threshold\n",
    "\n",
    "# chose the threshold that minimized the fpr and maximized the tpr for the training data,\n",
    "# and used that as the threshold used to determine the class to assign each client in testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "314ba38e-b5d3-49c6-8219-a071d42f811d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = pd.read_csv('gbm-implementation/batch_10_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3afd47f5-aa6e-494d-9c9c-261474596655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.8236666666666667), np.float64(0.789))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_init = pd.read_csv('gbm-implementation/batch_1_train.csv')\n",
    "\n",
    "predys = []\n",
    "\n",
    "init_log_odds = get_init['F0'][0] #F0\n",
    "for i in range(1, 101): \n",
    "    init_log_odds = init_log_odds + 0.1*test_results[f'F{i}']\n",
    "\n",
    "\n",
    "for i in init_log_odds:\n",
    "    if i >= 1000:\n",
    "        predys.append(1)\n",
    "    else:\n",
    "        pred_prob = np.exp(i) / (np.exp(i) + 1)\n",
    "        predys.append(pred_prob)\n",
    "\n",
    "test_results['predicted probs'] = predys\n",
    "\n",
    "\n",
    "pred_arr = [] \n",
    "for p in predys:\n",
    "    if p <= opt_threshold: \n",
    "        pred_arr.append(0)\n",
    "    if p > opt_threshold:\n",
    "        pred_arr.append(1)\n",
    "\n",
    "test_results['GBM predictions'] = pred_arr\n",
    "test_results['Baseline predictions'] = 0\n",
    "\n",
    "gb_accuracy = 1 - ((np.sum(np.abs(test_results['actual target'] - pred_arr))) / 6000)\n",
    "baseline_accuracy = 1 - (np.sum(np.abs(test_results['actual target'] - 0)) / 6000)\n",
    "\n",
    "gb_accuracy, baseline_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "137e4ae7-1766-4788-aed6-13cd897afd3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBM accuracy: 0.8236666666666667, Baseline_accuracy: 0.789\n",
      "GBM TPR: 0.33570300157977884, Baseline TPR: 0.0\n",
      "GBM FPR: 0.04583861427967892, Baseline FPR: 0.0\n",
      "GBM TNR: 0.954161385720321, Baseline TNR: 1.0\n",
      "GBM FNR: 0.6642969984202212, Baseline FNR: 1.0\n",
      "GBM Precision: 0.661993769470405, Baseline Precision: 0\n",
      "GBM F1 Score: 0.44549266247379454, Baseline F1 Score: 0\n"
     ]
    }
   ],
   "source": [
    "# created these functions simply to refresh memory on model performance metrics, will use built in sklearn libs for comparison section\n",
    "def get_TPR(data, target, pred):\n",
    "    tp_space = data[data[target] == 1]     \n",
    "    tp_df = tp_space[tp_space[pred] == 1]\n",
    "\n",
    "    tp_num = tp_df.shape[0]\n",
    "    tp_tot = tp_space.shape[0]\n",
    "    tpr = tp_num / tp_tot\n",
    "    \n",
    "    return [tpr, tp_num, tp_tot ]\n",
    "\n",
    "def get_FPR(data, target, pred):\n",
    "    fp_space = data[data[target] == 0]     \n",
    "    fp_df = fp_space[fp_space[pred] == 1]\n",
    "\n",
    "    fp_num = fp_df.shape[0]\n",
    "    fp_tot = fp_space.shape[0]\n",
    "    fpr = fp_num / fp_tot\n",
    "    return [fpr, fp_num, fp_tot]\n",
    "    \n",
    "\n",
    "baseline_fpr = get_FPR(test_results, 'actual target', 'Baseline predictions')[0]\n",
    "baseline_tpr = get_TPR(test_results, 'actual target', 'Baseline predictions')[0]\n",
    "\n",
    "gb_fpr = get_FPR(test_results, 'actual target', 'GBM predictions')[0]\n",
    "gb_tpr = get_TPR(test_results, 'actual target', 'GBM predictions')[0]\n",
    "\n",
    "gb_tp_num = get_TPR(test_results, 'actual target', 'GBM predictions')[1] \n",
    "gb_prec_tot = gb_tp_num + get_FPR(test_results, 'actual target', 'GBM predictions')[1]\n",
    "baseline_precision =  0 # 0/0\n",
    "gb_precision = gb_tp_num / gb_prec_tot\n",
    "\n",
    "baseline_f1 = 0\n",
    "gb_f1 = (2*gb_precision*gb_tpr) / (gb_precision + gb_tpr)\n",
    "\n",
    "print(f'GBM accuracy: {gb_accuracy}, Baseline_accuracy: {baseline_accuracy}')\n",
    "print(f'GBM TPR: {gb_tpr}, Baseline TPR: {baseline_tpr}')\n",
    "print(f'GBM FPR: {gb_fpr}, Baseline FPR: {baseline_fpr}')\n",
    "print(f'GBM TNR: {1 - gb_fpr}, Baseline TNR: {1 - baseline_fpr}')\n",
    "print(f'GBM FNR: {1 - gb_tpr}, Baseline FNR: {1 - baseline_tpr}')\n",
    "print(f'GBM Precision: {gb_precision}, Baseline Precision: {baseline_precision}')\n",
    "\n",
    "print(f'GBM F1 Score: {gb_f1}, Baseline F1 Score: {baseline_f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e9639e-7125-40ce-ad3b-10cfa723a711",
   "metadata": {},
   "source": [
    "### GBM Implementation vs. Scikit-learn's GBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4ca7833d-73e9-4c69-9cee-ee8752ee92e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3d010328-f9d2-469b-b517-34dee692f68b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(test_results['actual target'], test_results['GBM predictions'], labels = [0,1])\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = [0, 1])\n",
    "disp.plot()\n",
    "plt.title('GBM Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "65a87c6c-3c35-437b-aa29-e5f8e0bdf00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8cda69b0-4cbc-4575-b24c-42c4392f77f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pred_arr\n",
    "y_test = test_results['actual target'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "80a1ace6-6a7f-4d24-8ddd-1fa1a2ea36a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBM Implementation Accuracy: 0.8236666666666667\n",
      "\n",
      "GBM Implementation Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.95      0.90      4734\n",
      "           1       0.66      0.34      0.45      1266\n",
      "\n",
      "    accuracy                           0.82      6000\n",
      "   macro avg       0.75      0.64      0.67      6000\n",
      "weighted avg       0.80      0.82      0.80      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"GBM Implementation Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nGBM Implementation Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7713dd48-5cfb-4baa-bcdb-a8cb38a25bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/lib/python3.11/site-packages/sklearn/preprocessing/_label.py:110: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn GBC Accuracy: 0.8266666666666667\n",
      "\n",
      "Sklearn GBC Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.95      0.90      4734\n",
      "           1       0.66      0.37      0.47      1266\n",
      "\n",
      "    accuracy                           0.83      6000\n",
      "   macro avg       0.75      0.66      0.68      6000\n",
      "weighted avg       0.81      0.83      0.81      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "default_of_credit_card_clients = fetch_ucirepo(id=350)\n",
    "X = default_of_credit_card_clients.data.features \n",
    "y = default_of_credit_card_clients.data.targets \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 45)\n",
    "\n",
    "sklearn_gbm = GradientBoostingClassifier(n_estimators = 100, learning_rate = 0.05, max_depth = 2, random_state = 45)\n",
    "sklearn_gbm.fit(X_train, y_train)\n",
    "\n",
    "y_pred = sklearn_gbm.predict(X_test)\n",
    "\n",
    "# Evaluation: \n",
    "print(\"Sklearn GBC Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nSklearn GBC Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e7079c-c2d7-4b75-a164-db0c5aa7bbfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
