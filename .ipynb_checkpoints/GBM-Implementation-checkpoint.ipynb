{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30d6a011-f0d8-4ef5-a70a-f65d4e58eef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ucimlrepo\n",
      "  Using cached ucimlrepo-0.0.7-py3-none-any.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: pandas>=1.0.0 in /srv/conda/lib/python3.11/site-packages (from ucimlrepo) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2020.12.5 in /srv/conda/lib/python3.11/site-packages (from ucimlrepo) (2025.8.3)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /srv/conda/lib/python3.11/site-packages (from pandas>=1.0.0->ucimlrepo) (2.2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /srv/conda/lib/python3.11/site-packages (from pandas>=1.0.0->ucimlrepo) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /srv/conda/lib/python3.11/site-packages (from pandas>=1.0.0->ucimlrepo) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /srv/conda/lib/python3.11/site-packages (from pandas>=1.0.0->ucimlrepo) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /srv/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.17.0)\n",
      "Using cached ucimlrepo-0.0.7-py3-none-any.whl (8.0 kB)\n",
      "Installing collected packages: ucimlrepo\n",
      "Successfully installed ucimlrepo-0.0.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ucimlrepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9f15a39-b328-4d4a-ac90-73ab7fb25095",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import queue\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8e46540-7d0f-4a33-aaa9-e89ab723297e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to import dataset to python -- pasted from UCI ML repo \n",
    "\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "   \n",
    "default_of_credit_card_clients = fetch_ucirepo(id=350) \n",
    "\n",
    "default_cc = default_of_credit_card_clients.data.original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7631c22f-13b5-4b60-bdda-ef07d4652e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: One Hot Encoding qualitative features to be used for Regression Tree implementation\n",
    "def ohe_qual_feats(data):\n",
    "    qual_feat = ['X2', 'X3', 'X4']\n",
    "    for ql in qual_feat:\n",
    "        oh_test = data[[ql]]\n",
    "        ohe = OneHotEncoder(handle_unknown = 'ignore', sparse_output = False)\n",
    "        one_hot_encoded = ohe.fit_transform(oh_test)\n",
    "        one_hot_df = pd.DataFrame(one_hot_encoded, columns=ohe.get_feature_names_out([ql]))\n",
    "        data = pd.concat([data, one_hot_df], axis = 1)\n",
    "    data = data.drop(columns = ['X2', 'X3', 'X4'])\n",
    "    return data\n",
    "\n",
    "default_cc = ohe_qual_feats(default_cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4065766-48d2-44f7-94e3-8aee9b2b01e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating X and y tables from original table that is then split into a training and testing set\n",
    "\n",
    "X = default_cc.loc[:, default_cc.columns != 'Y'] \n",
    "y = default_cc.loc[:, 'Y'].to_frame(name = 'target')\n",
    "   \n",
    "default_of_credit_card_clients.data.original\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5cd0083d-c1e8-433e-b0d9-9609fc115702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating test_df which is the training data combined (so both the features and the target variable in one table)\n",
    "# This is to have the DTs to be created based on the true values (target) \n",
    "\n",
    "test_df = X_train\n",
    "test_df['target'] = y_train\n",
    "test_df = test_df.drop('ID', axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32bae034-22df-4c6e-8d0b-65e2ca4467e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for different points in a continuous feature, I wanted to find the boundary that minimizes the residuals\n",
    "# this method gathers 9 points (percentiles from the data)\n",
    "# I chose to do this over gradient descent simply because it is lest costly and would bring minimal change to the optimal boundary \n",
    "\n",
    "def minimize_stats(data, feat, target):\n",
    "    quant_df = data[[feat]] \n",
    "    qs = np.array(quant_df.quantile([0.1, 0.25, 0.3, 0.4, 0.5, 0.6, 0.75, 0.8, 0.9])[feat])\n",
    "    return qs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f19ce314-3ad4-40ad-bec4-6f960c4ce657",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# need this particular formula to convert leaf outputs to predicted probabilities\n",
    "\n",
    "def lodds_to_prob(l_odds):\n",
    "    pred_prob =  np.exp(l_odds) / (1 + np.exp(l_odds))\n",
    "    return pred_prob\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0509d7d3-2247-4cd0-adf3-36cd3b834e99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(11409.97122969299),\n",
       " 0,\n",
       " np.float64(-4.760636329592671e-13),\n",
       " 'X6',\n",
       "            X1  X5  X6  X7  X8  X9  X10  X11    X12    X13  ...  X3_2  X3_3  \\\n",
       " 25295   60000  24   0   0   0   0    0    0  50840  49592  ...   1.0   0.0   \n",
       " 23909   80000  26  -1   3   2  -1    2    2    495    330  ...   1.0   0.0   \n",
       " 25048  100000  54   0   0   0   0    5    4  37082  46041  ...   0.0   1.0   \n",
       " 17022   80000  23   0   0   0   0    0    0   6805   8449  ...   1.0   0.0   \n",
       " 5917   200000  46  -1  -1  -1  -1   -2   -2   1207   5590  ...   0.0   1.0   \n",
       " ...       ...  ..  ..  ..  ..  ..  ...  ...    ...    ...  ...   ...   ...   \n",
       " 12895   50000  25   0   0   0   0    0    0  50485  50397  ...   1.0   0.0   \n",
       " 28192  170000  34  -2  -2  -1   0    0   -2   1088   1088  ...   0.0   0.0   \n",
       " 6012   360000  47  -2  -2  -2  -2   -2   -2   1458     -4  ...   0.0   1.0   \n",
       " 6558   180000  30  -1  -1  -1  -1   -1   -2   1730   6792  ...   0.0   1.0   \n",
       " 23499  140000  33   1  -1  -1  -1   -1   -1    -23   5742  ...   1.0   0.0   \n",
       " \n",
       "        X3_4  X3_5  X3_6  X4_0  X4_1  X4_2  X4_3  target  \n",
       " 25295   0.0   0.0   0.0   0.0   0.0   1.0   0.0       0  \n",
       " 23909   0.0   0.0   0.0   0.0   1.0   0.0   0.0       0  \n",
       " 25048   0.0   0.0   0.0   0.0   1.0   0.0   0.0       1  \n",
       " 17022   0.0   0.0   0.0   0.0   0.0   1.0   0.0       0  \n",
       " 5917    0.0   0.0   0.0   0.0   1.0   0.0   0.0       1  \n",
       " ...     ...   ...   ...   ...   ...   ...   ...     ...  \n",
       " 12895   0.0   0.0   0.0   0.0   0.0   1.0   0.0       0  \n",
       " 28192   0.0   0.0   0.0   0.0   0.0   1.0   0.0       0  \n",
       " 6012    0.0   0.0   0.0   0.0   1.0   0.0   0.0       1  \n",
       " 6558    0.0   0.0   0.0   0.0   0.0   1.0   0.0       0  \n",
       " 23499   0.0   0.0   0.0   0.0   1.0   0.0   0.0       0  \n",
       " \n",
       " [24000 rows x 34 columns])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gets the residual reduction from a particular split of a feature\n",
    "# weights False Negatives to be 4 times \"more\" than False Positives\n",
    "# encourages residual trees to predict true positives correctly \n",
    "\n",
    "def get_rss(data, feat, target, bounds):\n",
    "    rss_arr = np.empty(len(bounds))\n",
    "    grad_arr = np.empty(len(bounds))\n",
    "    \n",
    "    for i, b in enumerate(bounds):\n",
    "        dat_l = data[data[feat] > b]\n",
    "        dat_u = data[data[feat] <= b]\n",
    "\n",
    "        mean_l = np.mean(dat_l[target])\n",
    "        mean_u = np.mean(dat_u[target])\n",
    "\n",
    "        rss_l = 0\n",
    "        rss_u = 0\n",
    "        \n",
    "        true_val_u = dat_u['target'].to_numpy()\n",
    "        true_val_l = dat_l['target'].to_numpy()\n",
    "\n",
    "        target_u = dat_u[target].to_numpy()\n",
    "        target_l = dat_l[target].to_numpy()\n",
    "        \n",
    "        for j, u in enumerate(true_val_u):\n",
    "            if u == 1:\n",
    "                rss_u += 4*((target_u[j] - mean_u)**2)\n",
    "            else:    \n",
    "                rss_u += ((target_u[j] - mean_u)**2)\n",
    "                \n",
    "        for k, l in enumerate(true_val_l):\n",
    "            if l == 1:\n",
    "                rss_l += 4*((target_l[k] - mean_l)**2)\n",
    "            else: \n",
    "                rss_l += ((target_l[k] - mean_l)**2)\n",
    "        \n",
    "        \n",
    "        \n",
    "        grad_l  = np.sum(-2*(dat_l[target]  - mean_l)) \n",
    "        grad_u = np.sum(-2*(dat_u[target] - mean_u))\n",
    "        \n",
    "        rss_arr[i] = rss_l + rss_u\n",
    "        grad_arr[i] = grad_l + grad_u\n",
    "\n",
    "    rss_min = np.min(rss_arr)\n",
    "    min_ind = np.argmin(rss_arr)\n",
    "    \n",
    "    # returns minimum rss, the optimal split, gradient, the feature name, and the orginal dataframe\n",
    "    return rss_min, bounds[min_ind], grad_arr[min_ind], feat, data\n",
    "\n",
    "get_rss(test_df, 'X6', 'target', [0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc8a50f0-c436-4d07-87ab-15c7cdcdbfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each point/node in tree, find the feature that minimizes the residual sum of squares\n",
    "# this will select the feature/split for each DT node\n",
    "# inputs are the data at that particular node, and target represents the feature that the DT is trying to predict\n",
    "\n",
    "def minimal_rss_quant(data, target):\n",
    "    feats = ['X1', 'X5', 'X6', 'X7', 'X8', 'X9', 'X10', 'X11', 'X12', 'X13', 'X14',\n",
    "       'X15', 'X16', 'X17', 'X18', 'X19', 'X20', 'X21', 'X22', 'X23', 'X2_1',\n",
    "       'X2_2', 'X3_0', 'X3_1', 'X3_2', 'X3_3', 'X3_4', 'X3_5', 'X3_6', 'X4_0',\n",
    "       'X4_1', 'X4_2', 'X4_3', target]\n",
    "    dater = []\n",
    "    min_rss = np.inf\n",
    "    min_stats = None\n",
    "    \n",
    "    for f in feats:\n",
    "        if f != target:\n",
    "            bounds = minimize_stats(data, f, target)\n",
    "            stats = get_rss(data, f, target, bounds)\n",
    "            rss = stats[0]\n",
    "\n",
    "            if rss < min_rss:\n",
    "                min_rss = rss\n",
    "                min_stats = stats\n",
    "    return min_stats\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4903c6c-8294-4f02-90f4-9db9a800b8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created DTNode to represent each node of the DTs created, \n",
    "# DecisionTreeRegressor class helps traverse the tree, create the tree, and \n",
    "# identify which nodes are considered leaves (will be useful later) \n",
    "\n",
    "class DTNode: \n",
    "\n",
    "    def __init__(self):\n",
    "        self.value = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "\n",
    "\n",
    "class DecisionTreeRegressor:\n",
    "\n",
    "    def __init__(self, data, depth, target):\n",
    "        self.depth = depth  \n",
    "        self.data = data\n",
    "        self.target = target\n",
    "\n",
    "    def create_tree(self):\n",
    "        self.dtree = build_tree(self.data, self.depth, self.target)\n",
    "        return self.dtree\n",
    "\n",
    "    def isLeaf(dtnode):\n",
    "        return ((dtnode.right is None) and (dtnode.left is None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4efdd9d8-14bf-46d3-acb1-89007cafc119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build_tree takes in the dataframe at this point (whatever split, the depth of the tree - as indicated when the DTRegressor class \n",
    "# is instantiated; uses recursion to continually split the new data split the most optimally, and continues until the indicated depth is reached\n",
    "\n",
    "def build_tree(data, depth, target):\n",
    "    if data.shape[0] > 0:\n",
    "        root = DTNode()\n",
    "        root.value = minimal_rss_quant(data, target)\n",
    "    else: \n",
    "        return 0 \n",
    " \n",
    "    if depth == 0:\n",
    "        return root  \n",
    "    else:\n",
    "        children_data = create_children(data, root)\n",
    "        \n",
    "        root.left = DTNode()\n",
    "        root.right = DTNode() \n",
    "\n",
    "        depth = depth - 1\n",
    "        root.left = build_tree(children_data[0], depth, target)\n",
    "        root.right = build_tree(children_data[1], depth, target)\n",
    "    return root\n",
    "\n",
    "# create_children is a helper function for build_tree in order to create new data frames upon splitting the data\n",
    "# e.g. if the feature to split on is X6, and the value is 1 then create_children takes the data and splits it into the following:\n",
    "# left node/dataframe : clients that have X6 <= 1 , right node/dataframe : clients that have X6 > 1\n",
    "\n",
    "def create_children(data, node):\n",
    "    feat_name = node.value[3]\n",
    "    pivot_val = node.value[1]\n",
    "    \n",
    "    left_data = data[data[feat_name] <= pivot_val]\n",
    "    right_data = data[data[feat_name] > pivot_val]\n",
    "\n",
    "    return left_data, right_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cdd3a1e6-aba1-4713-a2c6-708180bbd11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_new_val creates the new predictions for each leaf given the previous\n",
    "\n",
    "def get_new_val(data, resid, guess_col):\n",
    "    probs = lodds_to_prob(data[guess_col])\n",
    "    new_pred = np.sum(data[resid]) / np.sum(probs * (1 - probs))\n",
    "    return new_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "efcfe721-4995-44e5-8f2f-2bef2237b05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# leaf_arr[1]\n",
    "\n",
    "\n",
    "\n",
    "# for each decision tree,\n",
    "# find what value the output will predict\n",
    "# and then sum up those values F0 + v*sum blah blah (35:06)\n",
    "\n",
    "# Testing...\n",
    "\n",
    "# for each tree, store every node's split (less than + greater than/equal)\n",
    "# Run test data through the tree to get leaves, (in testing... store leaf values to be able to assign them to test)\n",
    "# Then repeat this with the next tree, with the predicted residuals/log(odds) from ^^^\n",
    "# have 2nd tree data stored, and assign leaves and repeat until dones with all trees\n",
    "# each observation shoudl now have some probability that it is 1 or 0, find probability threshold,, maybe use ROC or AUC to determine best threshold??\n",
    "# then assign final predicted values 0, 1\n",
    "# do model evaluations (confusion matrix, precision, recall, accuracy, F1 score, ROC curve\n",
    "\n",
    "#get_F0(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9072033-e59d-4f32-bc05-d0e8b6c17678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get initial prediction \n",
    "def get_F0(data): \n",
    "    p_1 = data[data['target'] == 1].shape[0] / len(data['target'])\n",
    "    odds = p_1 / (1 - p_1)\n",
    "    log_odds = np.log(odds)\n",
    "    return log_odds\n",
    "\n",
    "def lodds_to_prob(l_odds):\n",
    "    pred_prob =  np.exp(l_odds) / (1 + np.exp(l_odds))\n",
    "    return pred_prob\n",
    "\n",
    "class GBMInitializer:\n",
    "    def __init__(self, data, depth):\n",
    "        self.data = data\n",
    "        self.depth = depth\n",
    "        \n",
    "    def initialize_data(self):\n",
    "        lodds = get_F0(self.data)\n",
    "        prob_0 = lodds_to_prob(lodds)\n",
    "        self.data['F0'] = lodds\n",
    "        self.data['pred. prob 0'] = prob_0\n",
    "        self.data['residual 0'] = self.data['target'] - prob_0\n",
    "        return 0\n",
    "\n",
    "    def create_regression_tree(self, target):\n",
    "        fake_tree = DecisionTreeRegressor(self.data, self.depth, target)\n",
    "        doo = fake_tree.create_tree()\n",
    "        return doo\n",
    "\n",
    "    def create_model():\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "779a8f0b-b1b1-4577-89c8-08535c938d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isLeaf(dtnode):\n",
    "    return ((dtnode.right is None) and (dtnode.left is None))\n",
    "    \n",
    "def returnLeaves(root):\n",
    "    leaf_arr = [] \n",
    "    def getLeaves(root_node): \n",
    "        if isLeaf(root_node):\n",
    "            leaf_arr.append([root_node.value[4]])\n",
    "        else:\n",
    "            getLeaves(root_node.left)\n",
    "            getLeaves(root_node.right)\n",
    "    getLeaves(root)\n",
    "    return leaf_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a4c3d09-d77c-4fa8-9079-c331a557d126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>X11</th>\n",
       "      <th>X12</th>\n",
       "      <th>X13</th>\n",
       "      <th>...</th>\n",
       "      <th>X3_2</th>\n",
       "      <th>X3_3</th>\n",
       "      <th>X3_4</th>\n",
       "      <th>X3_5</th>\n",
       "      <th>X3_6</th>\n",
       "      <th>X4_0</th>\n",
       "      <th>X4_1</th>\n",
       "      <th>X4_2</th>\n",
       "      <th>X4_3</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25295</th>\n",
       "      <td>60000</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50840</td>\n",
       "      <td>49592</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23909</th>\n",
       "      <td>80000</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>495</td>\n",
       "      <td>330</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25048</th>\n",
       "      <td>100000</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>37082</td>\n",
       "      <td>46041</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17022</th>\n",
       "      <td>80000</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6805</td>\n",
       "      <td>8449</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5917</th>\n",
       "      <td>200000</td>\n",
       "      <td>46</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1207</td>\n",
       "      <td>5590</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12895</th>\n",
       "      <td>50000</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50485</td>\n",
       "      <td>50397</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28192</th>\n",
       "      <td>170000</td>\n",
       "      <td>34</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>1088</td>\n",
       "      <td>1088</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6012</th>\n",
       "      <td>360000</td>\n",
       "      <td>47</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1458</td>\n",
       "      <td>-4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6558</th>\n",
       "      <td>180000</td>\n",
       "      <td>30</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>1730</td>\n",
       "      <td>6792</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23499</th>\n",
       "      <td>140000</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-23</td>\n",
       "      <td>5742</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24000 rows Ã— 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           X1  X5  X6  X7  X8  X9  X10  X11    X12    X13  ...  X3_2  X3_3  \\\n",
       "25295   60000  24   0   0   0   0    0    0  50840  49592  ...   1.0   0.0   \n",
       "23909   80000  26  -1   3   2  -1    2    2    495    330  ...   1.0   0.0   \n",
       "25048  100000  54   0   0   0   0    5    4  37082  46041  ...   0.0   1.0   \n",
       "17022   80000  23   0   0   0   0    0    0   6805   8449  ...   1.0   0.0   \n",
       "5917   200000  46  -1  -1  -1  -1   -2   -2   1207   5590  ...   0.0   1.0   \n",
       "...       ...  ..  ..  ..  ..  ..  ...  ...    ...    ...  ...   ...   ...   \n",
       "12895   50000  25   0   0   0   0    0    0  50485  50397  ...   1.0   0.0   \n",
       "28192  170000  34  -2  -2  -1   0    0   -2   1088   1088  ...   0.0   0.0   \n",
       "6012   360000  47  -2  -2  -2  -2   -2   -2   1458     -4  ...   0.0   1.0   \n",
       "6558   180000  30  -1  -1  -1  -1   -1   -2   1730   6792  ...   0.0   1.0   \n",
       "23499  140000  33   1  -1  -1  -1   -1   -1    -23   5742  ...   1.0   0.0   \n",
       "\n",
       "       X3_4  X3_5  X3_6  X4_0  X4_1  X4_2  X4_3  target  \n",
       "25295   0.0   0.0   0.0   0.0   0.0   1.0   0.0       0  \n",
       "23909   0.0   0.0   0.0   0.0   1.0   0.0   0.0       0  \n",
       "25048   0.0   0.0   0.0   0.0   1.0   0.0   0.0       1  \n",
       "17022   0.0   0.0   0.0   0.0   0.0   1.0   0.0       0  \n",
       "5917    0.0   0.0   0.0   0.0   1.0   0.0   0.0       1  \n",
       "...     ...   ...   ...   ...   ...   ...   ...     ...  \n",
       "12895   0.0   0.0   0.0   0.0   0.0   1.0   0.0       0  \n",
       "28192   0.0   0.0   0.0   0.0   0.0   1.0   0.0       0  \n",
       "6012    0.0   0.0   0.0   0.0   1.0   0.0   0.0       1  \n",
       "6558    0.0   0.0   0.0   0.0   0.0   1.0   0.0       0  \n",
       "23499   0.0   0.0   0.0   0.0   1.0   0.0   0.0       0  \n",
       "\n",
       "[24000 rows x 34 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb28e337-ec05-4bef-8c0a-a3813667c7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_val(data, resid, probs):\n",
    "    new_pred = np.sum(data[resid]) / np.sum(data[probs] * (1 - data[probs]))\n",
    "    return new_pred\n",
    "\n",
    "\n",
    "class GradientBoostingMachine:\n",
    "    def __init__(self, data, num_learners, depth, dt_arr, learning_rate):\n",
    "        self.data = data\n",
    "        self.num_learners = num_learners\n",
    "        self.depth = depth\n",
    "        self.dt_arr = dt_arr\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def train(self): \n",
    "        gbm = GBMInitializer(self.data, self.depth)\n",
    "        gbm.initialize_data()\n",
    "        dt_tree = gbm.create_regression_tree('residual 0')\n",
    "        leaves = returnLeaves(dt_tree)\n",
    "\n",
    "        self.dt_arr.append(dt_tree)\n",
    "        pq_arr = []\n",
    "        for i in range(1, self.num_learners + 1):\n",
    "            new_leaves =[]\n",
    "            pq = queue.Queue()\n",
    "            for l in leaves:\n",
    "                l = l.copy()\n",
    "                new_pred = get_new_val(l[0], f'residual {i - 1}', f'pred. prob {i - 1}')\n",
    "                #l.append(new_pred) # new line.. see if this works\n",
    "                l[0].loc[:,f'F{i}'] = new_pred\n",
    "                new_leaves.append(l[0])\n",
    "                pq.put(new_pred)\n",
    "            \n",
    "            new_df = pd.concat(new_leaves, axis = 0)\n",
    "            odds = new_df[f'F{i - 1}'] + self.learning_rate*new_df[f'F{i}']\n",
    "            new_df[f'pred. prob {i}'] = lodds_to_prob(odds)\n",
    "            new_df[f'residual {i}'] = new_df['target'] - new_df[f'pred. prob {i}']\n",
    "\n",
    "            pq_arr.append(pq)\n",
    "            ore = GBMInitializer(new_df, 2)\n",
    "            dt_2 = ore.create_regression_tree(f'residual {i}')\n",
    "            self.dt_arr.append(dt_2)\n",
    "            leaves = returnLeaves(dt_2)\n",
    "        return self.dt_arr, new_df, pq_arr\n",
    "\n",
    "    \n",
    "    def train_next_batches(self, resid_num):\n",
    "        gbm = GBMInitializer(self.data, self.depth)\n",
    "        dt_tree2 = gbm.create_regression_tree(f'residual {resid_num}')\n",
    "        leaves = returnLeaves(dt_tree2)\n",
    "        \n",
    "        self.dt_arr.append(dt_tree2)\n",
    "        pq_arr2 = []\n",
    "        for i in range(resid_num + 1, resid_num + self.num_learners + 1):\n",
    "            new_leaves =[]\n",
    "            pq = queue.Queue()\n",
    "            for l in leaves:\n",
    "                l = l.copy()\n",
    "                new_pred = get_new_val(l[0], f'residual {i - 1}', f'pred. prob {i - 1}')\n",
    "                #l.append(new_pred) # new line.. see if this works\n",
    "                l[0].loc[:,f'F{i}'] = new_pred\n",
    "                new_leaves.append(l[0])\n",
    "                pq.put(new_pred)\n",
    "            \n",
    "            new_df = pd.concat(new_leaves, axis = 0)\n",
    "            odds = new_df[f'F{i - 1}'] + self.learning_rate*new_df[f'F{i}']\n",
    "            new_df[f'pred. prob {i}'] = lodds_to_prob(odds)\n",
    "            new_df[f'residual {i}'] = new_df['target'] - new_df[f'pred. prob {i}']\n",
    "\n",
    "            pq_arr2.append(pq)\n",
    "            ore = GBMInitializer(new_df, 2)\n",
    "            dt_2 = ore.create_regression_tree(f'residual {i}')\n",
    "            self.dt_arr.append(dt_2)\n",
    "            leaves = returnLeaves(dt_2)\n",
    "        return self.dt_arr, new_df, pq_arr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aed61fed-0c02-4480-b615-d694459ca31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to run the testing data on the trees created through training,\n",
    "# Made a traversal method to create the dataframes/leaves that result in the \n",
    "# Test data being run through the trees, then for each leaf, I assign the predicted\n",
    "# Values that were found in training (the leaf_queue) \n",
    "\n",
    "def traverse_tree(data, dtree, num_learner, leaf_queue, leaf_list):\n",
    "    \n",
    "    root = dtree\n",
    "    if isLeaf(root):\n",
    "        # stop process of splitting     \n",
    "        data.loc[:,f'F{num_learner}'] = leaf_queue.get()\n",
    "        leaf_list.append(data)\n",
    "        if leaf_queue.empty():\n",
    "            return 0\n",
    "    else:\n",
    "        split_val = dtree.value[1]\n",
    "        split_feat = dtree.value[3] \n",
    "\n",
    "        left_data = data[data[split_feat] <= split_val]\n",
    "        right_data = data[data[split_feat] > split_val]\n",
    "\n",
    "        traverse_tree(left_data, dtree.left, num_learner, leaf_queue, leaf_list)\n",
    "        traverse_tree(right_data, dtree.right, num_learner, leaf_queue, leaf_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33cd5b35-1526-45aa-9e14-5616e5796887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code is used to create new \"batches\" (10 new learners/regression trees)\n",
    "# In order to maintain JN's memory limit, each new csv file saves only the most recent prediction (log odds), probability, and residuals\n",
    "# as these are the only \"features\" needed to build the next sequential learner \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# init_df = pd.read_csv('batch_9_train.csv') # CHANGE\n",
    "# excluding_cols = []\n",
    "# resid = 90 # CHANGE\n",
    "# for i in range(resid): \n",
    "#     excluding_cols.append(f'F{i}')\n",
    "#     excluding_cols.append(f'residual {i}')\n",
    "#     excluding_cols.append(f'pred. prob {i}')\n",
    "# train_df = init_df.loc[:, ~init_df.columns.isin(excluding_cols)]\n",
    "\n",
    "# def train_test_batch(df, r_num, batch_num):\n",
    "#     batch = GradientBoostingMachine(df, num_learners = 10, depth = 2, dt_arr = [], learning_rate = 0.05)\n",
    "#     train_pred = batch.train_next_batches(resid_num = r_num)\n",
    "#     dt_arr_len = len(train_pred[0])\n",
    "#     dt_arr = train_pred[0][0:dt_arr_len - 1]\n",
    "#     pq_arr = train_pred[2]\n",
    "    \n",
    "#     batch_df = train_pred[1]\n",
    "#     batch_df.to_csv(f'batch_{batch_num}_train.csv', index = False)\n",
    "\n",
    "\n",
    "#     X_test = pd.read_csv(f'batch_{batch_num - 1}_test.csv')\n",
    "#     i = 0 \n",
    "#     n_estimate = r_num\n",
    "#     for d in dt_arr:\n",
    "#         leaf_list = [] \n",
    "#         traverse_tree(X_test, d, n_estimate + 1, pq_arr[i], leaf_list)\n",
    "#         X_test = pd.concat(leaf_list)\n",
    "#         i += 1\n",
    "#         n_estimate += 1\n",
    "    \n",
    "#     X_test.to_csv(f'batch_{batch_num}_test.csv', index = False)\n",
    "# train_test_batch(train_df, resid, 10) # CHANGE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db3ad2f-8944-453a-8a2f-d5a65482b3bf",
   "metadata": {},
   "source": [
    "### GBM Implementation Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2bd55c08-0057-4efd-85a4-dc59bdd93eb8",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'batch_10_test.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m train_results = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbatch_10_test.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m train_results\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/srv/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/srv/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/srv/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/srv/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/srv/conda/lib/python3.11/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'batch_10_test.csv'"
     ]
    }
   ],
   "source": [
    "train_results = pd.read_csv('batch_10_test.csv')\n",
    "train_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36aa714a-7040-4a77-8117-2d5bc6c27a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_arr = [] \n",
    "train_preds = train_results['pred. prob 100']\n",
    "def try_threshold(p):\n",
    "    for pred in train_preds:\n",
    "        if pred <= p: \n",
    "            pred_arr.append(0)\n",
    "        if pred > p:\n",
    "            pred_arr.append(1)\n",
    "    train_results['predicted target'] = pred_arr\n",
    "    \n",
    "    fpr_space = train_results[train_results['target'] == 0] \n",
    "    fps_pred = fpr_space[fpr_space['predicted target'] == 1]\n",
    "\n",
    "    tpr_space = train_results[train_results['target'] == 1] \n",
    "    tps_pred = tpr_space[tpr_space['predicted target'] == 0]\n",
    "    \n",
    "    fpr = fps_pred.shape[0] / fpr_space.shape[0]\n",
    "    tpr = tps_pred.shape[0] / tpr_space.shape[0]\n",
    "    return fpr, tpr\n",
    "\n",
    "\n",
    "unique_thresholds = train_results['pred. prob 100'].unique()\n",
    "fpr_tpr_arr = []\n",
    "for u in unique_thresholds:\n",
    "    pred_arr = []\n",
    "    train_preds = train_results['pred. prob 100']\n",
    "    fpr, tpr = try_threshold(u)\n",
    "    fpr_tpr_arr.append([u, fpr, tpr])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d379f4-a92b-4aff-86bc-8d44e744acd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_tpr_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e759119-9f96-41e2-8299-0d962b8a2eee",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fpr_tpr_arr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m opt_threshold = \u001b[43mfpr_tpr_arr\u001b[49m[\u001b[32m1\u001b[39m][\u001b[32m0\u001b[39m]\n\u001b[32m      2\u001b[39m opt_threshold\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# chose the threshold that minimized the fpr and maximized the tpr for the training data,\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# and used that as the threshold used to determine the class to assign each client in testing data\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'fpr_tpr_arr' is not defined"
     ]
    }
   ],
   "source": [
    "opt_threshold = fpr_tpr_arr[1][0]\n",
    "opt_threshold\n",
    "\n",
    "# chose the threshold that minimized the fpr and maximized the tpr for the training data,\n",
    "# and used that as the threshold used to determine the class to assign each client in testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314ba38e-b5d3-49c6-8219-a071d42f811d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = pd.read_csv('batch_10_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afd47f5-aa6e-494d-9c9c-261474596655",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_init = pd.read_csv('batch_1_train.csv')\n",
    "\n",
    "predys = []\n",
    "\n",
    "init_log_odds = get_init['F0'][0] #F0\n",
    "for i in range(1, 101): \n",
    "    init_log_odds = init_log_odds + 0.1*test_results[f'F{i}']\n",
    "\n",
    "\n",
    "for i in init_log_odds:\n",
    "    if i >= 1000:\n",
    "        predys.append(1)\n",
    "    else:\n",
    "        pred_prob = np.exp(i) / (np.exp(i) + 1)\n",
    "        predys.append(pred_prob)\n",
    "\n",
    "test_results['predicted probs'] = predys\n",
    "\n",
    "\n",
    "pred_arr = [] \n",
    "for p in predys:\n",
    "    if p <= opt_threshold: \n",
    "        pred_arr.append(0)\n",
    "    if p > opt_threshold:\n",
    "        pred_arr.append(1)\n",
    "\n",
    "test_results['GBM predictions'] = pred_arr\n",
    "test_results['Baseline predictions'] = 0\n",
    "\n",
    "gb_accuracy = 1 - ((np.sum(np.abs(test_results['actual target'] - pred_arr))) / 6000)\n",
    "baseline_accuracy = 1 - (np.sum(np.abs(test_results['actual target'] - 0)) / 6000)\n",
    "\n",
    "gb_accuracy, baseline_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137e4ae7-1766-4788-aed6-13cd897afd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# created these functions simply to refresh memory on model performance metrics, will use built in sklearn libs for comparison section\n",
    "def get_TPR(data, target, pred):\n",
    "    tp_space = data[data[target] == 1]     \n",
    "    tp_df = tp_space[tp_space[pred] == 1]\n",
    "\n",
    "    tp_num = tp_df.shape[0]\n",
    "    tp_tot = tp_space.shape[0]\n",
    "    tpr = tp_num / tp_tot\n",
    "    \n",
    "    return [tpr, tp_num, tp_tot ]\n",
    "\n",
    "def get_FPR(data, target, pred):\n",
    "    fp_space = data[data[target] == 0]     \n",
    "    fp_df = fp_space[fp_space[pred] == 1]\n",
    "\n",
    "    fp_num = fp_df.shape[0]\n",
    "    fp_tot = fp_space.shape[0]\n",
    "    fpr = fp_num / fp_tot\n",
    "    return [fpr, fp_num, fp_tot]\n",
    "    \n",
    "\n",
    "baseline_fpr = get_FPR(test_results, 'actual target', 'Baseline predictions')[0]\n",
    "baseline_tpr = get_TPR(test_results, 'actual target', 'Baseline predictions')[0]\n",
    "\n",
    "gb_fpr = get_FPR(test_results, 'actual target', 'GBM predictions')[0]\n",
    "gb_tpr = get_TPR(test_results, 'actual target', 'GBM predictions')[0]\n",
    "\n",
    "gb_tp_num = get_TPR(test_results, 'actual target', 'GBM predictions')[1] \n",
    "gb_prec_tot = gb_tp_num + get_FPR(test_results, 'actual target', 'GBM predictions')[1]\n",
    "baseline_precision =  0 # 0/0\n",
    "gb_precision = gb_tp_num / gb_prec_tot\n",
    "\n",
    "baseline_f1 = 0\n",
    "gb_f1 = (2*gb_precision*gb_tpr) / (gb_precision + gb_tpr)\n",
    "\n",
    "print(f'GBM accuracy: {gb_accuracy}, Baseline_accuracy: {baseline_accuracy}')\n",
    "print(f'GBM TPR: {gb_tpr}, Baseline TPR: {baseline_tpr}')\n",
    "print(f'GBM FPR: {gb_fpr}, Baseline FPR: {baseline_fpr}')\n",
    "print(f'GBM TNR: {1 - gb_fpr}, Baseline TNR: {1 - baseline_fpr}')\n",
    "print(f'GBM FNR: {1 - gb_tpr}, Baseline FNR: {1 - baseline_tpr}')\n",
    "print(f'GBM Precision: {gb_precision}, Baseline Precision: {baseline_precision}')\n",
    "\n",
    "print(f'GBM F1 Score: {gb_f1}, Baseline F1 Score: {baseline_f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e9639e-7125-40ce-ad3b-10cfa723a711",
   "metadata": {},
   "source": [
    "### GBM Implementation vs. Scikit-learn's GBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca7833d-73e9-4c69-9cee-ee8752ee92e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d010328-f9d2-469b-b517-34dee692f68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(test_results['actual target'], test_results['GBM predictions'], labels = [0,1])\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = [0, 1])\n",
    "disp.plot()\n",
    "plt.title('GBM Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a87c6c-3c35-437b-aa29-e5f8e0bdf00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cda69b0-4cbc-4575-b24c-42c4392f77f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pred_arr\n",
    "y_test = test_results['actual target'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a1ace6-6a7f-4d24-8ddd-1fa1a2ea36a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GBM Implementation Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nGBM Implementation Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7713dd48-5cfb-4baa-bcdb-a8cb38a25bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "default_of_credit_card_clients = fetch_ucirepo(id=350)\n",
    "X = default_of_credit_card_clients.data.features \n",
    "y = default_of_credit_card_clients.data.targets \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 45)\n",
    "\n",
    "sklearn_gbm = GradientBoostingClassifier(n_estimators = 100, learning_rate = 0.05, max_depth = 2, random_state = 45)\n",
    "sklearn_gbm.fit(X_train, y_train)\n",
    "\n",
    "y_pred = sklearn_gbm.predict(X_test)\n",
    "\n",
    "# Evaluation: \n",
    "print(\"Sklearn GBC Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nSklearn GBC Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
