{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "30d6a011-f0d8-4ef5-a70a-f65d4e58eef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ucimlrepo in /srv/conda/lib/python3.11/site-packages (0.0.7)\n",
      "Requirement already satisfied: pandas>=1.0.0 in /srv/conda/lib/python3.11/site-packages (from ucimlrepo) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2020.12.5 in /srv/conda/lib/python3.11/site-packages (from ucimlrepo) (2025.8.3)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /srv/conda/lib/python3.11/site-packages (from pandas>=1.0.0->ucimlrepo) (2.2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /srv/conda/lib/python3.11/site-packages (from pandas>=1.0.0->ucimlrepo) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /srv/conda/lib/python3.11/site-packages (from pandas>=1.0.0->ucimlrepo) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /srv/conda/lib/python3.11/site-packages (from pandas>=1.0.0->ucimlrepo) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /srv/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ucimlrepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c9f15a39-b328-4d4a-ac90-73ab7fb25095",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import queue\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e8e46540-7d0f-4a33-aaa9-e89ab723297e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to import dataset to python -- pasted from UCI ML repo \n",
    "\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "   \n",
    "default_of_credit_card_clients = fetch_ucirepo(id=350) \n",
    "\n",
    "default_cc = default_of_credit_card_clients.data.original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7631c22f-13b5-4b60-bdda-ef07d4652e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: One Hot Encoding qualitative features to be used for Regression Tree implementation\n",
    "def ohe_qual_feats(data):\n",
    "    qual_feat = ['X2', 'X3', 'X4']\n",
    "    for ql in qual_feat:\n",
    "        oh_test = data[[ql]]\n",
    "        ohe = OneHotEncoder(handle_unknown = 'ignore', sparse_output = False)\n",
    "        one_hot_encoded = ohe.fit_transform(oh_test)\n",
    "        one_hot_df = pd.DataFrame(one_hot_encoded, columns=ohe.get_feature_names_out([ql]))\n",
    "        data = pd.concat([data, one_hot_df], axis = 1)\n",
    "    data = data.drop(columns = ['X2', 'X3', 'X4'])\n",
    "    return data\n",
    "\n",
    "default_cc = ohe_qual_feats(default_cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d4065766-48d2-44f7-94e3-8aee9b2b01e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating X and y tables from original table that is then split into a training and testing set\n",
    "\n",
    "X = default_cc.loc[:, default_cc.columns != 'Y'] \n",
    "y = default_cc.loc[:, 'Y'].to_frame(name = 'target')\n",
    "   \n",
    "default_of_credit_card_clients.data.original\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5cd0083d-c1e8-433e-b0d9-9609fc115702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating test_df which is the training data combined (so both the features and the target variable in one table)\n",
    "# This is to have the DTs to be created based on the true values (target) \n",
    "\n",
    "test_df = X_train\n",
    "test_df['target'] = y_train\n",
    "test_df = test_df.drop('ID', axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "32bae034-22df-4c6e-8d0b-65e2ca4467e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for different points in a continuous feature, I wanted to find the boundary that minimizes the residuals\n",
    "# this method gathers 9 points (percentiles from the data)\n",
    "# I chose to do this over gradient descent simply because it is lest costly and would bring minimal change to the optimal boundary \n",
    "\n",
    "def minimize_stats(data, feat, target):\n",
    "    quant_df = data[[feat]] \n",
    "    qs = np.array(quant_df.quantile([0.1, 0.25, 0.3, 0.4, 0.5, 0.6, 0.75, 0.8, 0.9])[feat])\n",
    "    return qs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f19ce314-3ad4-40ad-bec4-6f960c4ce657",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# need this particular formula to convert leaf outputs to predicted probabilities\n",
    "\n",
    "def lodds_to_prob(l_odds):\n",
    "    pred_prob =  np.exp(l_odds) / (1 + np.exp(l_odds))\n",
    "    return pred_prob\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0509d7d3-2247-4cd0-adf3-36cd3b834e99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(11409.97122969299),\n",
       " 0,\n",
       " np.float64(-4.760636329592671e-13),\n",
       " 'X6',\n",
       "            X1  X5  X6  X7  X8  X9  X10  X11    X12    X13  ...  X3_2  X3_3  \\\n",
       " 25295   60000  24   0   0   0   0    0    0  50840  49592  ...   1.0   0.0   \n",
       " 23909   80000  26  -1   3   2  -1    2    2    495    330  ...   1.0   0.0   \n",
       " 25048  100000  54   0   0   0   0    5    4  37082  46041  ...   0.0   1.0   \n",
       " 17022   80000  23   0   0   0   0    0    0   6805   8449  ...   1.0   0.0   \n",
       " 5917   200000  46  -1  -1  -1  -1   -2   -2   1207   5590  ...   0.0   1.0   \n",
       " ...       ...  ..  ..  ..  ..  ..  ...  ...    ...    ...  ...   ...   ...   \n",
       " 12895   50000  25   0   0   0   0    0    0  50485  50397  ...   1.0   0.0   \n",
       " 28192  170000  34  -2  -2  -1   0    0   -2   1088   1088  ...   0.0   0.0   \n",
       " 6012   360000  47  -2  -2  -2  -2   -2   -2   1458     -4  ...   0.0   1.0   \n",
       " 6558   180000  30  -1  -1  -1  -1   -1   -2   1730   6792  ...   0.0   1.0   \n",
       " 23499  140000  33   1  -1  -1  -1   -1   -1    -23   5742  ...   1.0   0.0   \n",
       " \n",
       "        X3_4  X3_5  X3_6  X4_0  X4_1  X4_2  X4_3  target  \n",
       " 25295   0.0   0.0   0.0   0.0   0.0   1.0   0.0       0  \n",
       " 23909   0.0   0.0   0.0   0.0   1.0   0.0   0.0       0  \n",
       " 25048   0.0   0.0   0.0   0.0   1.0   0.0   0.0       1  \n",
       " 17022   0.0   0.0   0.0   0.0   0.0   1.0   0.0       0  \n",
       " 5917    0.0   0.0   0.0   0.0   1.0   0.0   0.0       1  \n",
       " ...     ...   ...   ...   ...   ...   ...   ...     ...  \n",
       " 12895   0.0   0.0   0.0   0.0   0.0   1.0   0.0       0  \n",
       " 28192   0.0   0.0   0.0   0.0   0.0   1.0   0.0       0  \n",
       " 6012    0.0   0.0   0.0   0.0   1.0   0.0   0.0       1  \n",
       " 6558    0.0   0.0   0.0   0.0   0.0   1.0   0.0       0  \n",
       " 23499   0.0   0.0   0.0   0.0   1.0   0.0   0.0       0  \n",
       " \n",
       " [24000 rows x 34 columns])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gets the residual reduction from a particular split of a feature\n",
    "# weights False Negatives to be 4 times \"more\" than False Positives\n",
    "# encourages residual trees to predict true positives correctly \n",
    "\n",
    "def get_rss(data, feat, target, bounds):\n",
    "    rss_arr = np.empty(len(bounds))\n",
    "    grad_arr = np.empty(len(bounds))\n",
    "    \n",
    "    for i, b in enumerate(bounds):\n",
    "        dat_l = data[data[feat] > b]\n",
    "        dat_u = data[data[feat] <= b]\n",
    "\n",
    "        mean_l = np.mean(dat_l[target])\n",
    "        mean_u = np.mean(dat_u[target])\n",
    "\n",
    "        rss_l = 0\n",
    "        rss_u = 0\n",
    "        \n",
    "        true_val_u = dat_u['target'].to_numpy()\n",
    "        true_val_l = dat_l['target'].to_numpy()\n",
    "\n",
    "        target_u = dat_u[target].to_numpy()\n",
    "        target_l = dat_l[target].to_numpy()\n",
    "        \n",
    "        for j, u in enumerate(true_val_u):\n",
    "            if u == 1:\n",
    "                rss_u += 4*((target_u[j] - mean_u)**2)\n",
    "            else:    \n",
    "                rss_u += ((target_u[j] - mean_u)**2)\n",
    "                \n",
    "        for k, l in enumerate(true_val_l):\n",
    "            if l == 1:\n",
    "                rss_l += 4*((target_l[k] - mean_l)**2)\n",
    "            else: \n",
    "                rss_l += ((target_l[k] - mean_l)**2)\n",
    "        \n",
    "        \n",
    "        \n",
    "        grad_l  = np.sum(-2*(dat_l[target]  - mean_l)) \n",
    "        grad_u = np.sum(-2*(dat_u[target] - mean_u))\n",
    "        \n",
    "        rss_arr[i] = rss_l + rss_u\n",
    "        grad_arr[i] = grad_l + grad_u\n",
    "\n",
    "    rss_min = np.min(rss_arr)\n",
    "    min_ind = np.argmin(rss_arr)\n",
    "    \n",
    "    # returns minimum rss, the optimal split, gradient, the feature name, and the orginal dataframe\n",
    "    return rss_min, bounds[min_ind], grad_arr[min_ind], feat, data\n",
    "\n",
    "get_rss(test_df, 'X6', 'target', [0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dc8a50f0-c436-4d07-87ab-15c7cdcdbfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each point/node in tree, find the feature that minimizes the residual sum of squares\n",
    "# this will select the feature/split for each DT node\n",
    "# inputs are the data at that particular node, and target represents the feature that the DT is trying to predict\n",
    "\n",
    "def minimal_rss_quant(data, target):\n",
    "    feats = ['X1', 'X5', 'X6', 'X7', 'X8', 'X9', 'X10', 'X11', 'X12', 'X13', 'X14',\n",
    "       'X15', 'X16', 'X17', 'X18', 'X19', 'X20', 'X21', 'X22', 'X23', 'X2_1',\n",
    "       'X2_2', 'X3_0', 'X3_1', 'X3_2', 'X3_3', 'X3_4', 'X3_5', 'X3_6', 'X4_0',\n",
    "       'X4_1', 'X4_2', 'X4_3', target]\n",
    "    dater = []\n",
    "    min_rss = np.inf\n",
    "    min_stats = None\n",
    "    \n",
    "    for f in feats:\n",
    "        if f != target:\n",
    "            bounds = minimize_stats(data, f, target)\n",
    "            stats = get_rss(data, f, target, bounds)\n",
    "            rss = stats[0]\n",
    "\n",
    "            if rss < min_rss:\n",
    "                min_rss = rss\n",
    "                min_stats = stats\n",
    "    return min_stats\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c4903c6c-8294-4f02-90f4-9db9a800b8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created DTNode to represent each node of the DTs created, \n",
    "# DecisionTreeRegressor class helps traverse the tree, create the tree, and \n",
    "# identify which nodes are considered leaves (will be useful later) \n",
    "\n",
    "class DTNode: \n",
    "\n",
    "    def __init__(self):\n",
    "        self.value = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "\n",
    "\n",
    "class DecisionTreeRegressor:\n",
    "\n",
    "    def __init__(self, data, depth, target):\n",
    "        self.depth = depth  \n",
    "        self.data = data\n",
    "        self.target = target\n",
    "\n",
    "    def create_tree(self):\n",
    "        self.dtree = build_tree(self.data, self.depth, self.target)\n",
    "        return self.dtree\n",
    "\n",
    "    def isLeaf(dtnode):\n",
    "        return ((dtnode.right is None) and (dtnode.left is None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4efdd9d8-14bf-46d3-acb1-89007cafc119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build_tree takes in the dataframe at this point (whatever split, the depth of the tree - as indicated when the DTRegressor class \n",
    "# is instantiated; uses recursion to continually split the new data split the most optimally, and continues until the indicated depth is reached\n",
    "\n",
    "def build_tree(data, depth, target):\n",
    "    if data.shape[0] > 0:\n",
    "        root = DTNode()\n",
    "        root.value = minimal_rss_quant(data, target)\n",
    "    else: \n",
    "        return 0 \n",
    " \n",
    "    if depth == 0:\n",
    "        return root  \n",
    "    else:\n",
    "        children_data = create_children(data, root)\n",
    "        \n",
    "        root.left = DTNode()\n",
    "        root.right = DTNode() \n",
    "\n",
    "        depth = depth - 1\n",
    "        root.left = build_tree(children_data[0], depth, target)\n",
    "        root.right = build_tree(children_data[1], depth, target)\n",
    "    return root\n",
    "\n",
    "# create_children is a helper function for build_tree in order to create new data frames upon splitting the data\n",
    "# e.g. if the feature to split on is X6, and the value is 1 then create_children takes the data and splits it into the following:\n",
    "# left node/dataframe : clients that have X6 <= 1 , right node/dataframe : clients that have X6 > 1\n",
    "\n",
    "def create_children(data, node):\n",
    "    feat_name = node.value[3]\n",
    "    pivot_val = node.value[1]\n",
    "    \n",
    "    left_data = data[data[feat_name] <= pivot_val]\n",
    "    right_data = data[data[feat_name] > pivot_val]\n",
    "\n",
    "    return left_data, right_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cdd3a1e6-aba1-4713-a2c6-708180bbd11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_new_val creates the new predictions for each leaf given the previous\n",
    "\n",
    "def get_new_val(data, resid, guess_col):\n",
    "    probs = lodds_to_prob(data[guess_col])\n",
    "    new_pred = np.sum(data[resid]) / np.sum(probs * (1 - probs))\n",
    "    return new_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "efcfe721-4995-44e5-8f2f-2bef2237b05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# leaf_arr[1]\n",
    "\n",
    "\n",
    "\n",
    "# for each decision tree,\n",
    "# find what value the output will predict\n",
    "# and then sum up those values F0 + v*sum blah blah (35:06)\n",
    "\n",
    "# Testing...\n",
    "\n",
    "# for each tree, store every node's split (less than + greater than/equal)\n",
    "# Run test data through the tree to get leaves, (in testing... store leaf values to be able to assign them to test)\n",
    "# Then repeat this with the next tree, with the predicted residuals/log(odds) from ^^^\n",
    "# have 2nd tree data stored, and assign leaves and repeat until dones with all trees\n",
    "# each observation shoudl now have some probability that it is 1 or 0, find probability threshold,, maybe use ROC or AUC to determine best threshold??\n",
    "# then assign final predicted values 0, 1\n",
    "# do model evaluations (confusion matrix, precision, recall, accuracy, F1 score, ROC curve\n",
    "\n",
    "#get_F0(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c9072033-e59d-4f32-bc05-d0e8b6c17678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get initial prediction \n",
    "def get_F0(data): \n",
    "    p_1 = data[data['target'] == 1].shape[0] / len(data['target'])\n",
    "    odds = p_1 / (1 - p_1)\n",
    "    log_odds = np.log(odds)\n",
    "    return log_odds\n",
    "\n",
    "def lodds_to_prob(l_odds):\n",
    "    pred_prob =  np.exp(l_odds) / (1 + np.exp(l_odds))\n",
    "    return pred_prob\n",
    "\n",
    "class GBMInitializer:\n",
    "    def __init__(self, data, depth):\n",
    "        self.data = data\n",
    "        self.depth = depth\n",
    "        \n",
    "    def initialize_data(self):\n",
    "        lodds = get_F0(self.data)\n",
    "        prob_0 = lodds_to_prob(lodds)\n",
    "        self.data['F0'] = lodds\n",
    "        self.data['pred. prob 0'] = prob_0\n",
    "        self.data['residual 0'] = self.data['target'] - prob_0\n",
    "        return 0\n",
    "\n",
    "    def create_regression_tree(self, target):\n",
    "        fake_tree = DecisionTreeRegressor(self.data, self.depth, target)\n",
    "        doo = fake_tree.create_tree()\n",
    "        return doo\n",
    "\n",
    "    def create_model():\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "779a8f0b-b1b1-4577-89c8-08535c938d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isLeaf(dtnode):\n",
    "    return ((dtnode.right is None) and (dtnode.left is None))\n",
    "    \n",
    "def returnLeaves(root):\n",
    "    leaf_arr = [] \n",
    "    def getLeaves(root_node): \n",
    "        if isLeaf(root_node):\n",
    "            leaf_arr.append([root_node.value[4]])\n",
    "        else:\n",
    "            getLeaves(root_node.left)\n",
    "            getLeaves(root_node.right)\n",
    "    getLeaves(root)\n",
    "    return leaf_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5a4c3d09-d77c-4fa8-9079-c331a557d126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>X11</th>\n",
       "      <th>X12</th>\n",
       "      <th>X13</th>\n",
       "      <th>...</th>\n",
       "      <th>X3_2</th>\n",
       "      <th>X3_3</th>\n",
       "      <th>X3_4</th>\n",
       "      <th>X3_5</th>\n",
       "      <th>X3_6</th>\n",
       "      <th>X4_0</th>\n",
       "      <th>X4_1</th>\n",
       "      <th>X4_2</th>\n",
       "      <th>X4_3</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25295</th>\n",
       "      <td>60000</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50840</td>\n",
       "      <td>49592</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23909</th>\n",
       "      <td>80000</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>495</td>\n",
       "      <td>330</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25048</th>\n",
       "      <td>100000</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>37082</td>\n",
       "      <td>46041</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17022</th>\n",
       "      <td>80000</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6805</td>\n",
       "      <td>8449</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5917</th>\n",
       "      <td>200000</td>\n",
       "      <td>46</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1207</td>\n",
       "      <td>5590</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12895</th>\n",
       "      <td>50000</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50485</td>\n",
       "      <td>50397</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28192</th>\n",
       "      <td>170000</td>\n",
       "      <td>34</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>1088</td>\n",
       "      <td>1088</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6012</th>\n",
       "      <td>360000</td>\n",
       "      <td>47</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1458</td>\n",
       "      <td>-4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6558</th>\n",
       "      <td>180000</td>\n",
       "      <td>30</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>1730</td>\n",
       "      <td>6792</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23499</th>\n",
       "      <td>140000</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-23</td>\n",
       "      <td>5742</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24000 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           X1  X5  X6  X7  X8  X9  X10  X11    X12    X13  ...  X3_2  X3_3  \\\n",
       "25295   60000  24   0   0   0   0    0    0  50840  49592  ...   1.0   0.0   \n",
       "23909   80000  26  -1   3   2  -1    2    2    495    330  ...   1.0   0.0   \n",
       "25048  100000  54   0   0   0   0    5    4  37082  46041  ...   0.0   1.0   \n",
       "17022   80000  23   0   0   0   0    0    0   6805   8449  ...   1.0   0.0   \n",
       "5917   200000  46  -1  -1  -1  -1   -2   -2   1207   5590  ...   0.0   1.0   \n",
       "...       ...  ..  ..  ..  ..  ..  ...  ...    ...    ...  ...   ...   ...   \n",
       "12895   50000  25   0   0   0   0    0    0  50485  50397  ...   1.0   0.0   \n",
       "28192  170000  34  -2  -2  -1   0    0   -2   1088   1088  ...   0.0   0.0   \n",
       "6012   360000  47  -2  -2  -2  -2   -2   -2   1458     -4  ...   0.0   1.0   \n",
       "6558   180000  30  -1  -1  -1  -1   -1   -2   1730   6792  ...   0.0   1.0   \n",
       "23499  140000  33   1  -1  -1  -1   -1   -1    -23   5742  ...   1.0   0.0   \n",
       "\n",
       "       X3_4  X3_5  X3_6  X4_0  X4_1  X4_2  X4_3  target  \n",
       "25295   0.0   0.0   0.0   0.0   0.0   1.0   0.0       0  \n",
       "23909   0.0   0.0   0.0   0.0   1.0   0.0   0.0       0  \n",
       "25048   0.0   0.0   0.0   0.0   1.0   0.0   0.0       1  \n",
       "17022   0.0   0.0   0.0   0.0   0.0   1.0   0.0       0  \n",
       "5917    0.0   0.0   0.0   0.0   1.0   0.0   0.0       1  \n",
       "...     ...   ...   ...   ...   ...   ...   ...     ...  \n",
       "12895   0.0   0.0   0.0   0.0   0.0   1.0   0.0       0  \n",
       "28192   0.0   0.0   0.0   0.0   0.0   1.0   0.0       0  \n",
       "6012    0.0   0.0   0.0   0.0   1.0   0.0   0.0       1  \n",
       "6558    0.0   0.0   0.0   0.0   0.0   1.0   0.0       0  \n",
       "23499   0.0   0.0   0.0   0.0   1.0   0.0   0.0       0  \n",
       "\n",
       "[24000 rows x 34 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eb28e337-ec05-4bef-8c0a-a3813667c7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_val(data, resid, probs):\n",
    "    new_pred = np.sum(data[resid]) / np.sum(data[probs] * (1 - data[probs]))\n",
    "    return new_pred\n",
    "\n",
    "\n",
    "class GradientBoostingMachine:\n",
    "    def __init__(self, data, num_learners, depth, dt_arr, learning_rate):\n",
    "        self.data = data\n",
    "        self.num_learners = num_learners\n",
    "        self.depth = depth\n",
    "        self.dt_arr = dt_arr\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def train(self): \n",
    "        gbm = GBMInitializer(self.data, self.depth)\n",
    "        gbm.initialize_data()\n",
    "        dt_tree = gbm.create_regression_tree('residual 0')\n",
    "        leaves = returnLeaves(dt_tree)\n",
    "\n",
    "        self.dt_arr.append(dt_tree)\n",
    "        pq_arr = []\n",
    "        for i in range(1, self.num_learners + 1):\n",
    "            new_leaves =[]\n",
    "            pq = queue.Queue()\n",
    "            for l in leaves:\n",
    "                l = l.copy()\n",
    "                new_pred = get_new_val(l[0], f'residual {i - 1}', f'pred. prob {i - 1}')\n",
    "                #l.append(new_pred) # new line.. see if this works\n",
    "                l[0].loc[:,f'F{i}'] = new_pred\n",
    "                new_leaves.append(l[0])\n",
    "                pq.put(new_pred)\n",
    "            \n",
    "            new_df = pd.concat(new_leaves, axis = 0)\n",
    "            odds = new_df[f'F{i - 1}'] + self.learning_rate*new_df[f'F{i}']\n",
    "            new_df[f'pred. prob {i}'] = lodds_to_prob(odds)\n",
    "            new_df[f'residual {i}'] = new_df['target'] - new_df[f'pred. prob {i}']\n",
    "\n",
    "            pq_arr.append(pq)\n",
    "            ore = GBMInitializer(new_df, 2)\n",
    "            dt_2 = ore.create_regression_tree(f'residual {i}')\n",
    "            self.dt_arr.append(dt_2)\n",
    "            leaves = returnLeaves(dt_2)\n",
    "        return self.dt_arr, new_df, pq_arr\n",
    "\n",
    "    \n",
    "    def train_next_batches(self, resid_num):\n",
    "        gbm = GBMInitializer(self.data, self.depth)\n",
    "        dt_tree2 = gbm.create_regression_tree(f'residual {resid_num}')\n",
    "        leaves = returnLeaves(dt_tree2)\n",
    "        \n",
    "        self.dt_arr.append(dt_tree2)\n",
    "        pq_arr2 = []\n",
    "        for i in range(resid_num + 1, resid_num + self.num_learners + 1):\n",
    "            new_leaves =[]\n",
    "            pq = queue.Queue()\n",
    "            for l in leaves:\n",
    "                l = l.copy()\n",
    "                new_pred = get_new_val(l[0], f'residual {i - 1}', f'pred. prob {i - 1}')\n",
    "                #l.append(new_pred) # new line.. see if this works\n",
    "                l[0].loc[:,f'F{i}'] = new_pred\n",
    "                new_leaves.append(l[0])\n",
    "                pq.put(new_pred)\n",
    "            \n",
    "            new_df = pd.concat(new_leaves, axis = 0)\n",
    "            odds = new_df[f'F{i - 1}'] + self.learning_rate*new_df[f'F{i}']\n",
    "            new_df[f'pred. prob {i}'] = lodds_to_prob(odds)\n",
    "            new_df[f'residual {i}'] = new_df['target'] - new_df[f'pred. prob {i}']\n",
    "\n",
    "            pq_arr2.append(pq)\n",
    "            ore = GBMInitializer(new_df, 2)\n",
    "            dt_2 = ore.create_regression_tree(f'residual {i}')\n",
    "            self.dt_arr.append(dt_2)\n",
    "            leaves = returnLeaves(dt_2)\n",
    "        return self.dt_arr, new_df, pq_arr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aed61fed-0c02-4480-b615-d694459ca31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to run the testing data on the trees created through training,\n",
    "# Made a traversal method to create the dataframes/leaves that result in the \n",
    "# Test data being run through the trees, then for each leaf, I assign the predicted\n",
    "# Values that were found in training (the leaf_queue) \n",
    "\n",
    "def traverse_tree(data, dtree, num_learner, leaf_queue, leaf_list):\n",
    "    \n",
    "    root = dtree\n",
    "    if isLeaf(root):\n",
    "        # stop process of splitting     \n",
    "        data.loc[:,f'F{num_learner}'] = leaf_queue.get()\n",
    "        leaf_list.append(data)\n",
    "        if leaf_queue.empty():\n",
    "            return 0\n",
    "    else:\n",
    "        split_val = dtree.value[1]\n",
    "        split_feat = dtree.value[3] \n",
    "\n",
    "        left_data = data[data[split_feat] <= split_val]\n",
    "        right_data = data[data[split_feat] > split_val]\n",
    "\n",
    "        traverse_tree(left_data, dtree.left, num_learner, leaf_queue, leaf_list)\n",
    "        traverse_tree(right_data, dtree.right, num_learner, leaf_queue, leaf_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "33cd5b35-1526-45aa-9e14-5616e5796887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code is used to create new \"batches\" (10 new learners/regression trees)\n",
    "# In order to maintain JN's memory limit, each new csv file saves only the most recent prediction (log odds), probability, and residuals\n",
    "# as these are the only \"features\" needed to build the next sequential learner \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# init_df = pd.read_csv('batch_9_train.csv') # CHANGE\n",
    "# excluding_cols = []\n",
    "# resid = 90 # CHANGE\n",
    "# for i in range(resid): \n",
    "#     excluding_cols.append(f'F{i}')\n",
    "#     excluding_cols.append(f'residual {i}')\n",
    "#     excluding_cols.append(f'pred. prob {i}')\n",
    "# train_df = init_df.loc[:, ~init_df.columns.isin(excluding_cols)]\n",
    "\n",
    "# def train_test_batch(df, r_num, batch_num):\n",
    "#     batch = GradientBoostingMachine(df, num_learners = 10, depth = 2, dt_arr = [], learning_rate = 0.05)\n",
    "#     train_pred = batch.train_next_batches(resid_num = r_num)\n",
    "#     dt_arr_len = len(train_pred[0])\n",
    "#     dt_arr = train_pred[0][0:dt_arr_len - 1]\n",
    "#     pq_arr = train_pred[2]\n",
    "    \n",
    "#     batch_df = train_pred[1]\n",
    "#     batch_df.to_csv(f'batch_{batch_num}_train.csv', index = False)\n",
    "\n",
    "\n",
    "#     X_test = pd.read_csv(f'batch_{batch_num - 1}_test.csv')\n",
    "#     i = 0 \n",
    "#     n_estimate = r_num\n",
    "#     for d in dt_arr:\n",
    "#         leaf_list = [] \n",
    "#         traverse_tree(X_test, d, n_estimate + 1, pq_arr[i], leaf_list)\n",
    "#         X_test = pd.concat(leaf_list)\n",
    "#         i += 1\n",
    "#         n_estimate += 1\n",
    "    \n",
    "#     X_test.to_csv(f'batch_{batch_num}_test.csv', index = False)\n",
    "# train_test_batch(train_df, resid, 10) # CHANGE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db3ad2f-8944-453a-8a2f-d5a65482b3bf",
   "metadata": {},
   "source": [
    "### GBM Implementation Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2bd55c08-0057-4efd-85a4-dc59bdd93eb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>X11</th>\n",
       "      <th>X12</th>\n",
       "      <th>X13</th>\n",
       "      <th>...</th>\n",
       "      <th>residual 97</th>\n",
       "      <th>F98</th>\n",
       "      <th>pred. prob 98</th>\n",
       "      <th>residual 98</th>\n",
       "      <th>F99</th>\n",
       "      <th>pred. prob 99</th>\n",
       "      <th>residual 99</th>\n",
       "      <th>F100</th>\n",
       "      <th>pred. prob 100</th>\n",
       "      <th>residual 100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>80000</td>\n",
       "      <td>31</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.376367</td>\n",
       "      <td>-0.853860</td>\n",
       "      <td>0.305534</td>\n",
       "      <td>-0.305534</td>\n",
       "      <td>0.025581</td>\n",
       "      <td>0.298892</td>\n",
       "      <td>-0.298892</td>\n",
       "      <td>-0.466154</td>\n",
       "      <td>0.500568</td>\n",
       "      <td>-0.500568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>320000</td>\n",
       "      <td>32</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.623633</td>\n",
       "      <td>-0.853860</td>\n",
       "      <td>0.305534</td>\n",
       "      <td>0.694466</td>\n",
       "      <td>0.025581</td>\n",
       "      <td>0.298892</td>\n",
       "      <td>0.701108</td>\n",
       "      <td>-0.466154</td>\n",
       "      <td>0.500568</td>\n",
       "      <td>0.499432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100000</td>\n",
       "      <td>28</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.376367</td>\n",
       "      <td>-0.853860</td>\n",
       "      <td>0.305534</td>\n",
       "      <td>-0.305534</td>\n",
       "      <td>0.025581</td>\n",
       "      <td>0.298892</td>\n",
       "      <td>-0.298892</td>\n",
       "      <td>-0.466154</td>\n",
       "      <td>0.500568</td>\n",
       "      <td>-0.500568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>310000</td>\n",
       "      <td>43</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.376367</td>\n",
       "      <td>-0.853860</td>\n",
       "      <td>0.305534</td>\n",
       "      <td>-0.305534</td>\n",
       "      <td>0.025581</td>\n",
       "      <td>0.298892</td>\n",
       "      <td>-0.298892</td>\n",
       "      <td>-0.466154</td>\n",
       "      <td>0.500568</td>\n",
       "      <td>-0.500568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>290000</td>\n",
       "      <td>44</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.376367</td>\n",
       "      <td>-0.853860</td>\n",
       "      <td>0.305534</td>\n",
       "      <td>-0.305534</td>\n",
       "      <td>0.025581</td>\n",
       "      <td>0.298892</td>\n",
       "      <td>-0.298892</td>\n",
       "      <td>-0.466154</td>\n",
       "      <td>0.500568</td>\n",
       "      <td>-0.500568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23995</th>\n",
       "      <td>150000</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>143947</td>\n",
       "      <td>147394</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.236609</td>\n",
       "      <td>0.299051</td>\n",
       "      <td>0.476765</td>\n",
       "      <td>-0.476765</td>\n",
       "      <td>-0.910598</td>\n",
       "      <td>0.563043</td>\n",
       "      <td>-0.563043</td>\n",
       "      <td>-1.166168</td>\n",
       "      <td>0.275098</td>\n",
       "      <td>-0.275098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23996</th>\n",
       "      <td>150000</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>156252</td>\n",
       "      <td>121999</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.236609</td>\n",
       "      <td>0.299051</td>\n",
       "      <td>0.476765</td>\n",
       "      <td>-0.476765</td>\n",
       "      <td>-0.910598</td>\n",
       "      <td>0.563043</td>\n",
       "      <td>-0.563043</td>\n",
       "      <td>-1.166168</td>\n",
       "      <td>0.275098</td>\n",
       "      <td>-0.275098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23997</th>\n",
       "      <td>130000</td>\n",
       "      <td>46</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>79705</td>\n",
       "      <td>59716</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.236609</td>\n",
       "      <td>0.299051</td>\n",
       "      <td>0.476765</td>\n",
       "      <td>-0.476765</td>\n",
       "      <td>-0.910598</td>\n",
       "      <td>0.563043</td>\n",
       "      <td>-0.563043</td>\n",
       "      <td>-1.166168</td>\n",
       "      <td>0.275098</td>\n",
       "      <td>-0.275098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23998</th>\n",
       "      <td>220000</td>\n",
       "      <td>37</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>189976</td>\n",
       "      <td>189665</td>\n",
       "      <td>...</td>\n",
       "      <td>0.763391</td>\n",
       "      <td>0.299051</td>\n",
       "      <td>0.476765</td>\n",
       "      <td>0.523235</td>\n",
       "      <td>-0.910598</td>\n",
       "      <td>0.563043</td>\n",
       "      <td>0.436957</td>\n",
       "      <td>-1.166168</td>\n",
       "      <td>0.275098</td>\n",
       "      <td>0.724902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23999</th>\n",
       "      <td>240000</td>\n",
       "      <td>41</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>239633</td>\n",
       "      <td>242710</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.236609</td>\n",
       "      <td>0.299051</td>\n",
       "      <td>0.476765</td>\n",
       "      <td>-0.476765</td>\n",
       "      <td>-0.293104</td>\n",
       "      <td>0.570624</td>\n",
       "      <td>-0.570624</td>\n",
       "      <td>-1.166168</td>\n",
       "      <td>0.413040</td>\n",
       "      <td>-0.413040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24000 rows × 67 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           X1  X5  X6  X7  X8  X9  X10  X11     X12     X13  ...  residual 97  \\\n",
       "0       80000  31  -2  -2  -2  -2   -2   -2       0       0  ...    -0.376367   \n",
       "1      320000  32  -2  -2  -2  -2   -2   -2      -1      -1  ...     0.623633   \n",
       "2      100000  28  -2  -2  -2  -2   -1   -1       0       0  ...    -0.376367   \n",
       "3      310000  43  -2  -2  -2  -2   -2   -2       0       0  ...    -0.376367   \n",
       "4      290000  44  -2  -2  -2  -2   -2   -2       0       0  ...    -0.376367   \n",
       "...       ...  ..  ..  ..  ..  ..  ...  ...     ...     ...  ...          ...   \n",
       "23995  150000  42   2   0   0   0    0    0  143947  147394  ...    -0.236609   \n",
       "23996  150000  25   2   0   0   0    0    0  156252  121999  ...    -0.236609   \n",
       "23997  130000  46   2   0   0   0    0    0   79705   59716  ...    -0.236609   \n",
       "23998  220000  37   2   2   2   0    0    0  189976  189665  ...     0.763391   \n",
       "23999  240000  41   2   2   2   2    2    2  239633  242710  ...    -0.236609   \n",
       "\n",
       "            F98  pred. prob 98  residual 98       F99  pred. prob 99  \\\n",
       "0     -0.853860       0.305534    -0.305534  0.025581       0.298892   \n",
       "1     -0.853860       0.305534     0.694466  0.025581       0.298892   \n",
       "2     -0.853860       0.305534    -0.305534  0.025581       0.298892   \n",
       "3     -0.853860       0.305534    -0.305534  0.025581       0.298892   \n",
       "4     -0.853860       0.305534    -0.305534  0.025581       0.298892   \n",
       "...         ...            ...          ...       ...            ...   \n",
       "23995  0.299051       0.476765    -0.476765 -0.910598       0.563043   \n",
       "23996  0.299051       0.476765    -0.476765 -0.910598       0.563043   \n",
       "23997  0.299051       0.476765    -0.476765 -0.910598       0.563043   \n",
       "23998  0.299051       0.476765     0.523235 -0.910598       0.563043   \n",
       "23999  0.299051       0.476765    -0.476765 -0.293104       0.570624   \n",
       "\n",
       "       residual 99      F100  pred. prob 100  residual 100  \n",
       "0        -0.298892 -0.466154        0.500568     -0.500568  \n",
       "1         0.701108 -0.466154        0.500568      0.499432  \n",
       "2        -0.298892 -0.466154        0.500568     -0.500568  \n",
       "3        -0.298892 -0.466154        0.500568     -0.500568  \n",
       "4        -0.298892 -0.466154        0.500568     -0.500568  \n",
       "...            ...       ...             ...           ...  \n",
       "23995    -0.563043 -1.166168        0.275098     -0.275098  \n",
       "23996    -0.563043 -1.166168        0.275098     -0.275098  \n",
       "23997    -0.563043 -1.166168        0.275098     -0.275098  \n",
       "23998     0.436957 -1.166168        0.275098      0.724902  \n",
       "23999    -0.570624 -1.166168        0.413040     -0.413040  \n",
       "\n",
       "[24000 rows x 67 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_results = pd.read_csv('batch_10_train.csv')\n",
    "train_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "36aa714a-7040-4a77-8117-2d5bc6c27a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_arr = [] \n",
    "train_preds = train_results['pred. prob 100']\n",
    "def try_threshold(p):\n",
    "    for pred in train_preds:\n",
    "        if pred <= p: \n",
    "            pred_arr.append(0)\n",
    "        if pred > p:\n",
    "            pred_arr.append(1)\n",
    "    train_results['predicted target'] = pred_arr\n",
    "    \n",
    "    fpr_space = train_results[train_results['target'] == 0] \n",
    "    fps_pred = fpr_space[fpr_space['predicted target'] == 1]\n",
    "\n",
    "    tpr_space = train_results[train_results['target'] == 1] \n",
    "    tps_pred = tpr_space[tpr_space['predicted target'] == 0]\n",
    "    \n",
    "    fpr = fps_pred.shape[0] / fpr_space.shape[0]\n",
    "    tpr = tps_pred.shape[0] / tpr_space.shape[0]\n",
    "    return fpr, tpr\n",
    "\n",
    "\n",
    "unique_thresholds = train_results['pred. prob 100'].unique()\n",
    "fpr_tpr_arr = []\n",
    "for u in unique_thresholds:\n",
    "    pred_arr = []\n",
    "    train_preds = train_results['pred. prob 100']\n",
    "    fpr, tpr = try_threshold(u)\n",
    "    fpr_tpr_arr.append([u, fpr, tpr])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "04d379f4-a92b-4aff-86bc-8d44e744acd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[np.float64(0.5005682443626555), 0.0, 1.0],\n",
       " [np.float64(0.3619798957544952), 0.10316693505099302, 0.6577281191806331],\n",
       " [np.float64(0.28213291519667), 0.23687600644122384, 0.3972067039106145],\n",
       " [np.float64(0.4215504005499201), 0.06011808910359635, 0.8175046554934823],\n",
       " [np.float64(0.2781720814655709), 0.33263553408480945, 0.3191806331471136],\n",
       " [np.float64(0.4167686208021049), 0.06333870101986044, 0.813780260707635],\n",
       " [np.float64(0.3744625938194448), 0.09382716049382717, 0.746927374301676],\n",
       " [np.float64(0.2931271694420759), 0.2170155662909286, 0.5312849162011173],\n",
       " [np.float64(0.4346877760393332), 0.05067096081588835, 0.9195530726256983],\n",
       " [np.float64(0.353936155359063), 0.21690821256038648, 0.5316573556797021],\n",
       " [np.float64(0.2750984523396937), 0.9994632313472893, 0.0005586592178770949],\n",
       " [np.float64(0.4130398773359012), 0.09377348362855609, 0.746927374301676]]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpr_tpr_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3e759119-9f96-41e2-8299-0d962b8a2eee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.3619798957544952)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_threshold = fpr_tpr_arr[1][0]\n",
    "opt_threshold\n",
    "\n",
    "# chose the threshold that minimized the fpr and maximized the tpr for the training data,\n",
    "# and used that as the threshold used to determine the class to assign each client in testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "314ba38e-b5d3-49c6-8219-a071d42f811d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = pd.read_csv('batch_10_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3afd47f5-aa6e-494d-9c9c-261474596655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.8236666666666667), np.float64(0.789))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_init = pd.read_csv('batch_1_train.csv')\n",
    "\n",
    "predys = []\n",
    "\n",
    "init_log_odds = get_init['F0'][0] #F0\n",
    "for i in range(1, 101): \n",
    "    init_log_odds = init_log_odds + 0.1*test_results[f'F{i}']\n",
    "\n",
    "\n",
    "for i in init_log_odds:\n",
    "    if i >= 1000:\n",
    "        predys.append(1)\n",
    "    else:\n",
    "        pred_prob = np.exp(i) / (np.exp(i) + 1)\n",
    "        predys.append(pred_prob)\n",
    "\n",
    "test_results['predicted probs'] = predys\n",
    "\n",
    "\n",
    "pred_arr = [] \n",
    "for p in predys:\n",
    "    if p <= opt_threshold: \n",
    "        pred_arr.append(0)\n",
    "    if p > opt_threshold:\n",
    "        pred_arr.append(1)\n",
    "\n",
    "test_results['GBM predictions'] = pred_arr\n",
    "test_results['Baseline predictions'] = 0\n",
    "\n",
    "gb_accuracy = 1 - ((np.sum(np.abs(test_results['actual target'] - pred_arr))) / 6000)\n",
    "baseline_accuracy = 1 - (np.sum(np.abs(test_results['actual target'] - 0)) / 6000)\n",
    "\n",
    "gb_accuracy, baseline_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "137e4ae7-1766-4788-aed6-13cd897afd3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBM accuracy: 0.8236666666666667, Baseline_accuracy: 0.789\n",
      "GBM TPR: 0.33570300157977884, Baseline TPR: 0.0\n",
      "GBM FPR: 0.04583861427967892, Baseline FPR: 0.0\n",
      "GBM TNR: 0.954161385720321, Baseline TNR: 1.0\n",
      "GBM FNR: 0.6642969984202212, Baseline FNR: 1.0\n",
      "GBM Precision: 0.661993769470405, Baseline Precision: 0\n",
      "GBM F1 Score: 0.44549266247379454, Baseline F1 Score: 0\n"
     ]
    }
   ],
   "source": [
    "# created these functions simply to refresh memory on model performance metrics, will use built in sklearn libs for comparison section\n",
    "def get_TPR(data, target, pred):\n",
    "    tp_space = data[data[target] == 1]     \n",
    "    tp_df = tp_space[tp_space[pred] == 1]\n",
    "\n",
    "    tp_num = tp_df.shape[0]\n",
    "    tp_tot = tp_space.shape[0]\n",
    "    tpr = tp_num / tp_tot\n",
    "    \n",
    "    return [tpr, tp_num, tp_tot ]\n",
    "\n",
    "def get_FPR(data, target, pred):\n",
    "    fp_space = data[data[target] == 0]     \n",
    "    fp_df = fp_space[fp_space[pred] == 1]\n",
    "\n",
    "    fp_num = fp_df.shape[0]\n",
    "    fp_tot = fp_space.shape[0]\n",
    "    fpr = fp_num / fp_tot\n",
    "    return [fpr, fp_num, fp_tot]\n",
    "    \n",
    "\n",
    "baseline_fpr = get_FPR(test_results, 'actual target', 'Baseline predictions')[0]\n",
    "baseline_tpr = get_TPR(test_results, 'actual target', 'Baseline predictions')[0]\n",
    "\n",
    "gb_fpr = get_FPR(test_results, 'actual target', 'GBM predictions')[0]\n",
    "gb_tpr = get_TPR(test_results, 'actual target', 'GBM predictions')[0]\n",
    "\n",
    "gb_tp_num = get_TPR(test_results, 'actual target', 'GBM predictions')[1] \n",
    "gb_prec_tot = gb_tp_num + get_FPR(test_results, 'actual target', 'GBM predictions')[1]\n",
    "baseline_precision =  0 # 0/0\n",
    "gb_precision = gb_tp_num / gb_prec_tot\n",
    "\n",
    "baseline_f1 = 0\n",
    "gb_f1 = (2*gb_precision*gb_tpr) / (gb_precision + gb_tpr)\n",
    "\n",
    "print(f'GBM accuracy: {gb_accuracy}, Baseline_accuracy: {baseline_accuracy}')\n",
    "print(f'GBM TPR: {gb_tpr}, Baseline TPR: {baseline_tpr}')\n",
    "print(f'GBM FPR: {gb_fpr}, Baseline FPR: {baseline_fpr}')\n",
    "print(f'GBM TNR: {1 - gb_fpr}, Baseline TNR: {1 - baseline_fpr}')\n",
    "print(f'GBM FNR: {1 - gb_tpr}, Baseline FNR: {1 - baseline_tpr}')\n",
    "print(f'GBM Precision: {gb_precision}, Baseline Precision: {baseline_precision}')\n",
    "\n",
    "print(f'GBM F1 Score: {gb_f1}, Baseline F1 Score: {baseline_f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e9639e-7125-40ce-ad3b-10cfa723a711",
   "metadata": {},
   "source": [
    "### GBM Implementation vs. Scikit-learn's GBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4ca7833d-73e9-4c69-9cee-ee8752ee92e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "65a87c6c-3c35-437b-aa29-e5f8e0bdf00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8cda69b0-4cbc-4575-b24c-42c4392f77f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_imp = pred_arr\n",
    "y_test_imp = test_results['actual target'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b7061e01-faa1-4bb6-a8de-2e30353d0332",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/lib/python3.11/site-packages/sklearn/preprocessing/_label.py:110: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "default_of_credit_card_clients = fetch_ucirepo(id=350)\n",
    "X = default_of_credit_card_clients.data.features \n",
    "y = default_of_credit_card_clients.data.targets \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 45)\n",
    "\n",
    "sklearn_gbm = GradientBoostingClassifier(n_estimators = 100, learning_rate = 0.05, max_depth = 2, random_state = 45)\n",
    "sklearn_gbm.fit(X_train, y_train)\n",
    "\n",
    "y_pred = sklearn_gbm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7a74fc8c-c092-4549-ab6e-498be9265c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBM Implementation Accuracy: 0.8236666666666667\n",
      "Sklearn GBC Accuracy: 0.8266666666666667\n",
      "\n",
      "GBM Implementation Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.95      0.90      4734\n",
      "           1       0.66      0.34      0.45      1266\n",
      "\n",
      "    accuracy                           0.82      6000\n",
      "   macro avg       0.75      0.64      0.67      6000\n",
      "weighted avg       0.80      0.82      0.80      6000\n",
      "\n",
      "\n",
      "Sklearn GBC Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.95      0.90      4734\n",
      "           1       0.66      0.37      0.47      1266\n",
      "\n",
      "    accuracy                           0.83      6000\n",
      "   macro avg       0.75      0.66      0.68      6000\n",
      "weighted avg       0.81      0.83      0.81      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation: \n",
    "print(\"GBM Implementation Accuracy:\", accuracy_score(y_test_imp, y_pred_imp))\n",
    "print(\"Sklearn GBC Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(\"\\nGBM Implementation Classification Report:\\n\", classification_report(y_test_imp, y_pred_imp))\n",
    "print(\"\\nSklearn GBC Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e8f58f-ade8-49be-a84b-ba86dde472a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_imp = confusion_matrix(y_test_imp, y_pred_imp, labels = [0,1])\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred, labels = [0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e8e07ea3-3986-486f-993c-3b497d3a2879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7c686ba04c10>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, axes = plt.subplots(1,2, sharex = True, sharey = True, figsize = (12, 4))\n",
    "\n",
    "plot_1 = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = [0, 1])\n",
    "plot_1.plot(ax = axes[0])\n",
    "\n",
    "plot_2 = ConfusionMatrixDisplay(confusion_matrix = cm_imp, display_labels = [0, 1])\n",
    "plot_2.plot(ax = axes[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17637471-cb53-40ac-8ca9-dec5f5fc35ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
